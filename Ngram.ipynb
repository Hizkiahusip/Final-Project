{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit9682afc1dce14a5c9def90105ba5095f",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "C:\\Users\\Kijeng\\AppData\\Local\\Programs\\Python\\Python38\\lib\\csv.py\n"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from tkinter import *\n",
    "\n",
    "import string\n",
    "import time\n",
    "import gc\n",
    "from math import log10\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "start_time = time.time()\n",
    "import csv\n",
    "import pandas as pd\n",
    "print(csv.__file__)\n",
    "\n",
    "import glob, os\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys, time\n",
    "from itertools import cycle\n",
    "\n",
    "global keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengecek kalimat yang tidak ada di korpus kata\n",
    "def cekWrong(conn,inputWrong):\n",
    "    x = 0\n",
    "\n",
    "#   memecah inputan menjadi per kata\n",
    "    temp_l = inputWrong.split()\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "#   Membersihkan inputan\n",
    "    for word in temp_l :\n",
    "        j = 0\n",
    "        for l in word :\n",
    "            if l in string.punctuation:\n",
    "                if l == \"'\":\n",
    "                    if j+1<len(word) and word[j+1] == 's':\n",
    "                        j = j + 1\n",
    "                        continue\n",
    "                word = word.replace(l,\" \")\n",
    "#                     print(j,word[j])\n",
    "            j += 1\n",
    "        temp_l[i] = word.lower()\n",
    "        i=i+1   \n",
    "\n",
    "    content = \" \".join(temp_l)\n",
    "    token = content.split()\n",
    "    \n",
    "#   cek perkata ada di korpus kata atau tidak\n",
    "    for teks in token:\n",
    "        kata.append(teks)\n",
    "        teksStemming = fetch(teks)\n",
    "        cur = conn.cursor()\n",
    "#       query untuk cek yang tidak memiliki tag\n",
    "        cur.execute('Select kata from korpus_kata where kata = (?) and tag != \"Entri tidak ditemukan\" and tag != \"tidak ditemukan kata yang dicari\" and tag != \"entri tidak ditemukan\" ',(teksStemming,))\n",
    "        a = cur.fetchone()\n",
    "        conn.commit()\n",
    "        if a==None:\n",
    "#           kaliamt yang tidak ada di korpus kata disimpan di wrong\n",
    "            wrong.append(x)\n",
    "        x= x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk mengambil 3 kata sebelum kalimat singkatan\n",
    "def kalimatTeks(i):\n",
    "    n = wrong[i]\n",
    "    if n >= 3 :\n",
    "        temp = kata[n-3] + ' ' + kata[n-2]  + ' ' + kata[n-1]\n",
    "        input_teks.append(temp)\n",
    "    elif n == 2 :\n",
    "        temp = 's' + ' ' + kata[n-2] + ' ' + kata[n-1]\n",
    "        input_teks.append(temp)\n",
    "    elif n == 1 :\n",
    "        temp = 's' + ' ' + 's' + ' ' + kata[n-1]\n",
    "        input_teks.append(temp)\n",
    "    elif n == 0 :\n",
    "        temp = 's' + ' ' + 's' + ' ' + 's'\n",
    "        input_teks.append(temp)\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi membersihkan kalimat\n",
    "def removePunctuations(sen):\n",
    "    temp_l = sen.split()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "#   menghilangkan tanda baca dan membuat jadi huruf kecil\n",
    "    for word in temp_l :\n",
    "        j = 0\n",
    "        for l in word :\n",
    "            if l in string.punctuation:\n",
    "                if l == \"'\":\n",
    "                    if j+1<len(word) and word[j+1] == 's':\n",
    "                        j = j + 1\n",
    "                        continue\n",
    "                word = word.replace(l,\" \")\n",
    "            j += 1\n",
    "\n",
    "        temp_l[i] = word.lower()\n",
    "        i=i+1   \n",
    "        \n",
    "    content = \" \".join(temp_l)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memuat korpus kalimat untuk menjadi dataset dan menghitung jumlah dari quadgram, trigram dan bigram \n",
    "def loadCorpus(file_path, bi_dict, tri_dict, quad_dict, vocab_dict):\n",
    "\n",
    "    w1 = ''    #variabel untuk menyimpan 3 kalimat terakhir untuk token set\n",
    "    w2 = ''    #variabel untuk menyimpan 2 kalimat terakhir untuk token set\n",
    "    w3 = ''    #variabel untuk menyimpan kalimat terakhir untuk token set\n",
    "    token = []\n",
    "\n",
    "    word_len = 0\n",
    "\n",
    "#   memuat korpus kalimat dan di baca line per line\n",
    "    with open(file_path,'r', encoding=\"utf-8\") as file:\n",
    "        print(file)\n",
    "        for line in file:\n",
    "            print(line)\n",
    "#           memecah kalimat menjadi kata\n",
    "            temp_l = line.split()\n",
    "            i = 0\n",
    "            j = 0\n",
    "            \n",
    "#           menghilangkan tanda baca dan mebuat kata menjadi huruf kecil\n",
    "            for word in temp_l :\n",
    "                j = 0\n",
    "                for l in word :\n",
    "                    if l in string.punctuation:\n",
    "                        if l == \"'\":\n",
    "                            if j+1<len(word) and word[j+1] == 's':\n",
    "                                j = j + 1\n",
    "                                continue\n",
    "                        word = word.replace(l,\" \")\n",
    "                    j += 1\n",
    "\n",
    "                temp_l[i] = word.lower()\n",
    "                i=i+1   \n",
    "\n",
    "            content = \" \".join(temp_l)\n",
    "\n",
    "            token = content.split()\n",
    "#           menghitung jumlah kata\n",
    "            word_len = word_len + len(token)  \n",
    "\n",
    "            if not token:\n",
    "                continue\n",
    "\n",
    "#           menambahkan kalimat terakhir ke variabel\n",
    "            if w3!= '':\n",
    "                token.insert(0,w3)\n",
    "            \n",
    "#           token untuk bigrams\n",
    "            temp0 = list(ngrams(token,2))\n",
    "\n",
    "#           menambahkan kalimat dua terakhir ke variabel\n",
    "            if w2!= '':\n",
    "                token.insert(0,w2)\n",
    "\n",
    "#           token untuk trigrams\n",
    "            temp1 = list(ngrams(token,3))\n",
    "\n",
    "#           menambahkan kalimat tiga terakhir ke variabel\n",
    "            if w1!= '':\n",
    "                token.insert(0,w1)\n",
    "\n",
    "#           menambahkan kata unik ke vocaulary\n",
    "            for word in token:\n",
    "                if word not in vocab_dict:\n",
    "                    vocab_dict[word] = 1\n",
    "                else:\n",
    "                    vocab_dict[word]+= 1\n",
    "                    \n",
    "#           token untuk quadgrams\n",
    "            temp2 = list(ngrams(token,4))\n",
    "\n",
    "    \n",
    "#           menghitung frekuensi dari bigram\n",
    "            for t in temp0:\n",
    "                sen = ' '.join(t)\n",
    "                bi_dict[sen] += 1\n",
    "\n",
    "#           menghitung frekuensi dari trigram\n",
    "            for t in temp1:\n",
    "                sen = ' '.join(t)\n",
    "                tri_dict[sen] += 1\n",
    "\n",
    "#           menghitung frekuensi dari quadgrams\n",
    "            for t in temp2:\n",
    "                sen = ' '.join(t)\n",
    "                quad_dict[sen] += 1\n",
    "\n",
    "#           menghitung panjang token\n",
    "            n = len(token)\n",
    "\n",
    "#           menambahkan kalimat ke variabel\n",
    "            if (n -3) >= 0:\n",
    "                w1 = token[n -3]\n",
    "            if (n -2) >= 0:\n",
    "                w2 = token[n -2]\n",
    "            if (n -1) >= 0:\n",
    "                w3 = token[n -1]\n",
    "    return word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memuat korpus kalimat untuk menjadi dataset character dan menghitung jumlah dari quadgram, trigram dan bigram \n",
    "def loadCorpusChar(file_path, bi_dict_char, tri_dict_char, quad_dict_char, vocab_dict_char):\n",
    "\n",
    "    w1_char = ''    #variabel untuk menyimpan 3 huruf terakhir untuk token set\n",
    "    w2_char = ''    #variabel untuk menyimpan 2 huruf terakhir untuk token set\n",
    "    w3_char = ''    #variabel untuk menyimpan huruf terakhir untuk token set\n",
    "    token = []\n",
    "    \n",
    "    word_len_char = 0\n",
    "\n",
    "#   memuat korpus kalimat dan di baca line per line\n",
    "    with open(file_path,'r', encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "\n",
    "#           memecah kalimat menjadi karakter\n",
    "            temp_l_char = list(line)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            \n",
    "#           menghilangkan tanda baca dan mebuat huruf kecil\n",
    "            for char in temp_l_char :\n",
    "                j = 0\n",
    "                for l in char :\n",
    "                    if l in string.punctuation:\n",
    "                        if l == \"'\":\n",
    "                            if j+1<len(char) and char[j+1] == 's':\n",
    "                                j = j + 1\n",
    "                                continue\n",
    "                        char = char.replace(l,\" \")\n",
    "                        #print(j,word[j])\n",
    "                    j += 1\n",
    "\n",
    "                temp_l_char[i] = char.lower()\n",
    "                i=i+1   \n",
    "\n",
    "\n",
    "            content_char = \" \".join(temp_l_char)\n",
    "\n",
    "            token_char = content_char.split()\n",
    "#           menghitung jumlah karakter\n",
    "            word_len_char = word_len_char + len(token_char)  \n",
    "\n",
    "            if not token_char:\n",
    "                continue\n",
    "\n",
    "#           menambahkan huruf terakhir ke variabel\n",
    "            if w3_char!= '':\n",
    "                token_char.insert(0,w3_char)\n",
    "\n",
    "#           token untuk bigrams\n",
    "            temp0_char = list(ngrams(token_char,2))\n",
    "\n",
    "#           menambahkan 2 huruf terakhir ke variabel\n",
    "            if w2_char!= '':\n",
    "                token_char.insert(0,w2_char)\n",
    "\n",
    "#           token untuk trigrams\n",
    "            temp1_char = list(ngrams(token_char,3))\n",
    "\n",
    "#           menambahkan 3 huruf terakhir ke variabel\n",
    "            if w1_char!= '':\n",
    "                token_char.insert(0,w1_char)\n",
    "\n",
    "#           menambahkan huruf unik ke vocaulary\n",
    "            for char in token_char:\n",
    "                if char not in vocab_dict_char:\n",
    "                    vocab_dict_char[char] = 1\n",
    "                else:\n",
    "                    vocab_dict_char[char]+= 1\n",
    "                  \n",
    "                \n",
    "#           token untuk quadgrams\n",
    "            temp2_char = list(ngrams(token_char,4))\n",
    "\n",
    "#           menghitung frekuensi dari bigram\n",
    "            for t in temp0_char:\n",
    "                sen_char = ''.join(t)\n",
    "                bi_dict_char[sen_char] += 1\n",
    "\n",
    "#           menghitung frekuensi dari trigram\n",
    "            for t in temp1_char:\n",
    "                sen_char = ''.join(t)\n",
    "                tri_dict_char[sen_char] += 1\n",
    "\n",
    "#           menghitung frekuensi dari quadgram\n",
    "            for t in temp2_char:\n",
    "                sen_char = ''.join(t)\n",
    "                quad_dict_char[sen_char] += 1\n",
    "\n",
    "\n",
    "#           menghitung panjang token\n",
    "            n_char = len(token_char)\n",
    "\n",
    "#           menambahkan kalimat ke variabel\n",
    "            if (n_char -3) >= 0:\n",
    "                w1_char = token_char[n_char -3]\n",
    "            if (n_char -2) >= 0:\n",
    "                w2_char = token_char[n_char -2]\n",
    "            if (n_char -1) >= 0:\n",
    "                w3_char = token_char[n_char -1]\n",
    "    return word_len_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat dict untuk menyimpan probabilitas kalimat quadgrams\n",
    "def findQuadgramProbGT(vocab_dict, bi_dict, tri_dict, quad_dict, quad_prob_dict, nc_dict, k):\n",
    "    \n",
    "    i = 0\n",
    "    V = len(vocab_dict)\n",
    "\n",
    "    for quad_sen in quad_dict:\n",
    "#       membagi kaliamat quadgrams menjadi perkata\n",
    "        quad_token = quad_sen.split()\n",
    "        \n",
    "#       membuat kalimat trigram dari kalimat quadgrams\n",
    "        tri_sen = ' '.join(quad_token[:3])\n",
    "\n",
    "        quad_count = quad_dict[quad_sen]\n",
    "        tri_count = tri_dict[tri_sen]\n",
    "        \n",
    "#       menggunakan goot turing smoothing\n",
    "        if quad_dict[quad_sen] <= k  or (quad_sen not in quad_dict):\n",
    "            quad_count = findGoodTuringAdjustCount( quad_dict[quad_sen], k, nc_dict)\n",
    "        if tri_dict[tri_sen] <= k  or (tri_sen not in tri_dict):\n",
    "            tri_count = findGoodTuringAdjustCount( tri_dict[tri_sen], k, nc_dict)\n",
    "\n",
    "#       menghitung probabilitas quadgram dengan membagi kalimat quadgrams dengan trigram\n",
    "        prob = quad_count / tri_count\n",
    "        \n",
    "#       menyimpan probabilitas quadgrams ke dict\n",
    "        if tri_sen not in quad_prob_dict:\n",
    "            quad_prob_dict[tri_sen] = []\n",
    "            quad_prob_dict[tri_sen].append([prob,quad_token[-1]])\n",
    "        else:\n",
    "            quad_prob_dict[tri_sen].append([prob,quad_token[-1]])\n",
    "  \n",
    "    prob = None\n",
    "    quad_token = None\n",
    "    tri_sen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat dict untuk menyimpan probabilitas kalimat trigrams\n",
    "def findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, nc_dict, k):\n",
    "    V = len(vocab_dict)\n",
    "\n",
    "    for tri in tri_dict:\n",
    "#       membagi kaliamat trigrams menjadi perkata\n",
    "        tri_token = tri.split()\n",
    "\n",
    "#       membuat kalimat bigram dari kalimat trigrams\n",
    "        bi_sen = ' '.join(tri_token[:2])\n",
    "        \n",
    "        tri_count = tri_dict[tri]\n",
    "        bi_count = bi_dict[bi_sen]\n",
    "        \n",
    "#       menggunakan goot turing smoothing\n",
    "        if tri_dict[tri] <= k or (tri not in tri_dict):\n",
    "            tri_count = findGoodTuringAdjustCount( tri_dict[tri], k, nc_dict)\n",
    "        if bi_dict[bi_sen] <= k or (bi_sen not in bi_dict):\n",
    "            bi_count = findGoodTuringAdjustCount( bi_dict[bi_sen], k, nc_dict)\n",
    "\n",
    "#       menghitung probabilitas trigram dengan membagi kalimat trigram dengan bigram\n",
    "        prob = tri_count / bi_count\n",
    "        \n",
    "#       menyimpan probabilitas trigrams ke dict\n",
    "        if bi_sen not in tri_prob_dict:\n",
    "            tri_prob_dict[bi_sen] = []\n",
    "            tri_prob_dict[bi_sen].append([prob,tri_token[-1]])\n",
    "        else:\n",
    "            tri_prob_dict[bi_sen].append([prob,tri_token[-1]])\n",
    "    \n",
    "    prob = None\n",
    "    tri_token = None\n",
    "    bi_sen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat dict untuk menyimpan probabilitas kalimat bigrams\n",
    "def findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, nc_dict, k):\n",
    "    V = len(vocab_dict)\n",
    "    bigram = []\n",
    "    bigram_prob = []\n",
    "    \n",
    "    for bi in bi_dict:\n",
    "#       membagi kaliamat bigrams menjadi perkata\n",
    "        bi_token = bi.split()\n",
    "        \n",
    "        bigram.append(bi)\n",
    "        \n",
    "#       membuat kalimat unigrams dari kalimat bigrams\n",
    "        unigram = bi_token[0]\n",
    "\n",
    "        bi_count = bi_dict[bi]\n",
    "        uni_count = vocab_dict[unigram]\n",
    "        \n",
    "#       menggunakan goot turing smoothing\n",
    "        if bi_dict[bi] <= k or (bi not in bi_dict):\n",
    "            bi_count = findGoodTuringAdjustCount( bi_dict[bi], k, nc_dict)\n",
    "        if vocab_dict[unigram] <= k or (unigram not in vocab_dict):\n",
    "            uni_count = findGoodTuringAdjustCount( vocab_dict[unigram], k, nc_dict)\n",
    "        \n",
    "#       menghitung probabilitas bigram dengan membagi kalimat bigram dengan unigram\n",
    "        prob = bi_count / uni_count\n",
    "        bigram_prob.append(prob)\n",
    "        \n",
    "#       menyimpan probabilitas bigrams ke dict\n",
    "        if unigram not in bi_prob_dict:\n",
    "            bi_prob_dict[unigram] = []\n",
    "            bi_prob_dict[unigram].append([prob,bi_token[-1]])\n",
    "        else:\n",
    "            bi_prob_dict[unigram].append([prob,bi_token[-1]])\n",
    "\n",
    "    prob = None\n",
    "    bi_token = None\n",
    "    unigram = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat dict untuk menyimpan probabilitas karakter trigrams\n",
    "def findCharTrigramProbGT(vocab_dict_char, bi_dict_char, tri_dict_char, tri_prob_dict_char, nc_dict_char, k):\n",
    "    V = len(vocab_dict_char)\n",
    "    trigram = []\n",
    "    trigram_prob = []    \n",
    "    \n",
    "    for tri_char in tri_dict_char:\n",
    "#       membagi kaliamat trigrams menjadi perhuruf\n",
    "        tri_token_char = tri_char.split()\n",
    "        char = ' '.join(tri_token_char[0])\n",
    "        tri_tokenc = char.split()\n",
    "        \n",
    "#       membuat huruf bigram dari huruf trigrams\n",
    "        bi_sen_char = ''.join(tri_tokenc[:2])\n",
    "        \n",
    "        tri_count_char = tri_dict_char[tri_char]\n",
    "        bi_count_char = bi_dict_char[bi_sen_char]\n",
    "        \n",
    "#       menggunakan goot turing smoothing\n",
    "        if tri_dict_char[tri_char] <= k or (tri_char not in tri_dict_char):\n",
    "            tri_count_char = findGoodTuringAdjustCount( tri_dict_char[tri_char], k, nc_dict_char)\n",
    "        if bi_dict_char[bi_sen_char] <= k or (bi_sen_char not in bi_dict_char):\n",
    "            bi_count_char = findGoodTuringAdjustCount( bi_dict_char[bi_sen_char], k, nc_dict_char)\n",
    "        \n",
    "#       menghitung probabilitas trigram dengan membagi huruf trigram dengan bigram\n",
    "        prob_char = tri_count_char / bi_count_char\n",
    "        trigram_prob.append(prob_char)\n",
    "        tri_prob_dict_char[tri_char] = prob_char        \n",
    "        \n",
    "#       menyimpan probabilitas trigrams ke dict\n",
    "        if bi_sen_char not in tri_prob_dict_char:\n",
    "            tri_prob_dict_char[bi_sen_char] = []\n",
    "            tri_prob_dict_char[bi_sen_char].append([prob_char,tri_token_char[-1]])\n",
    "        else:\n",
    "            tri_prob_dict_char[bi_sen_char].append([prob_char,tri_token_char[-1]])\n",
    "    \n",
    "    prob_char = None\n",
    "    tri_token_char = None\n",
    "    bi_sen_char = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat dict untuk menyimpan probabilitas karakter bigrams\n",
    "def findCharBigramProbGT(vocab_dict_char, bi_dict_char, bi_prob_dict_char, nc_dict_char, k):\n",
    "           \n",
    "    #vocabulary size\n",
    "    V = len(vocab_dict_char)\n",
    "    bigram = []\n",
    "    bigram_prob = []\n",
    "    for bi_char in bi_dict_char:\n",
    "#       membagi kaliamat bigrams menjadi perhuruf\n",
    "        bi_token_char = bi_char.split()\n",
    "        \n",
    "        bigram.append(bi_char)\n",
    "        \n",
    "#       membuat huruf unigram dari huruf bigrams\n",
    "        unigram_char = bi_token_char[0][0]\n",
    "        test.append(unigram_char)\n",
    "\n",
    "\n",
    "        bi_count_char = bi_dict_char[bi_char]\n",
    "        uni_count_char = vocab_dict_char[unigram_char]\n",
    "\n",
    "#       menggunakan goot turing smoothing\n",
    "        if bi_dict_char[bi_char] <= k or (bi_char not in bi_dict_char):\n",
    "            bi_count_char = findGoodTuringAdjustCount( bi_dict_char[bi_char], k, nc_dict_char)\n",
    "        if vocab_dict_char[unigram_char] <= k or (unigram_char not in vocab_dict_char):\n",
    "            uni_count_char = findGoodTuringAdjustCount( vocab_dict_char[unigram_char], k, nc_dict_char)\n",
    "        \n",
    "#       menghitung probabilitas bigram dengan membagi huruf bigram dengan unigram\n",
    "        prob_char = bi_count_char / uni_count_char\n",
    "        bigram_prob.append(prob_char)\n",
    "        bi_prob_dict_char[bi_char] = prob_char\n",
    "\n",
    "#       menyimpan probabilitas bigrams ke dict\n",
    "        if unigram_char not in bi_prob_dict_char:\n",
    "            bi_prob_dict_char[unigram_char] = []\n",
    "            bi_prob_dict_char[unigram_char].append([prob_char,bi_token_char[-1]])\n",
    "        else:\n",
    "            bi_prob_dict_char[unigram_char].append([prob_char,bi_token_char[-1]])\n",
    "    \n",
    "    prob_char = None\n",
    "    bi_token_char = None\n",
    "    unigram_char = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk mengurutkan probabilitas dari yang terbesar\n",
    "def sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict):\n",
    "    for key in bi_prob_dict:\n",
    "        if len(bi_prob_dict[key])>1:\n",
    "            bi_prob_dict[key] = sorted(bi_prob_dict[key],reverse = True)\n",
    "    \n",
    "    for key in tri_prob_dict:\n",
    "        if len(tri_prob_dict[key])>1:\n",
    "            tri_prob_dict[key] = sorted(tri_prob_dict[key],reverse = True)\n",
    "    \n",
    "    for key in quad_prob_dict:\n",
    "        if len(quad_prob_dict[key])>1:\n",
    "            quad_prob_dict[key] = sorted(quad_prob_dict[key],reverse = True)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk mengambil inputan\n",
    "def takeInput():\n",
    "    cond = False\n",
    "    while(cond == False):\n",
    "        sen = input('Enter the string\\n')\n",
    "        temp = sen.split()\n",
    "        if len(temp) < 0:\n",
    "            print(\"Please enter atleast 1 words !\")\n",
    "        else:\n",
    "            cond = True\n",
    "            temp = temp[:]\n",
    "    sen = \" \".join(temp)\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-06a7878d105d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "# menghitung fit line terbaik dari simple regression\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import style\n",
    "\n",
    "# menghitung slope dari fit line terbaik\n",
    "def findBestFitSlope(x,y):\n",
    "    m = (( mean(x)*mean(y) - mean(x*y) ) / \n",
    "          ( mean(x)** 2 - mean(x**2)))\n",
    "\n",
    "    return m\n",
    "      \n",
    "# menghitung intercept dari fit line terbaik\n",
    "def findBestFitIntercept(x,y,m):\n",
    "    c = mean(y) - m*mean(x)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# menghitung nc untuk quadgrams dan trigrams dimana c > 5\n",
    "def findFrequencyOfFrequencyCount(ngram_dict, k, n, V, token_len):\n",
    "    #for keeping count of 'c' value i.e Nc\n",
    "#   untuk tetap menghitung dari nilai c\n",
    "    nc_dict = {}\n",
    "    \n",
    "#   menghitung nilai dari Nc,c = 0  dengan v^n - (total ngram tokens)\n",
    "    nc_dict[0] = V**n - token_len\n",
    "#   menghitung nc dimana c = k, disini menggunakan k = 5\n",
    "\n",
    "#   menghitung dari ngram\n",
    "    for key in ngram_dict:\n",
    "        if ngram_dict[key] <= k + 1:\n",
    "            if ngram_dict[key] not in nc_dict:\n",
    "                nc_dict[ ngram_dict[key]] = 1\n",
    "            else:\n",
    "                nc_dict[ ngram_dict[key] ] += 1\n",
    "    \n",
    "    \n",
    "#   check jika semua value dari nc berada di nc_dict atau tidak\n",
    "    val_present = True\n",
    "    for i in range(1,7):\n",
    "        if i not in nc_dict:\n",
    "            val_present = False\n",
    "            break\n",
    "    if val_present == True:\n",
    "        return nc_dict\n",
    "    \n",
    "#   mengisi nilai dari nc dimana menggunakan regressi di atas c = 6\n",
    "\n",
    "    data_pts = {}\n",
    "    i = 0\n",
    "#   untuk quadgrams\n",
    "    for key in ngram_dict:\n",
    "        if ngram_dict[key] not in data_pts:\n",
    "                data_pts[ ngram_dict[key] ] = 1\n",
    "                i += 1\n",
    "        if i >5:\n",
    "            break\n",
    "            \n",
    "#   mendapat nilai nc \n",
    "    for key in ngram_dict:\n",
    "        if ngram_dict[key] in data_pts:\n",
    "            data_pts[ ngram_dict[key] ] += 1\n",
    "    \n",
    "#   membuat coordinat x, y dari regresi\n",
    "    x_coor = [ np.log(item) for item in data_pts ]\n",
    "    y_coor = [ np.log( data_pts[item] ) for item in data_pts ]\n",
    "    x = np.array(x_coor, dtype = np.float64)\n",
    "    y = np.array(y_coor , dtype = np.float64)\n",
    "  \n",
    "    slope_m = findBestFitSlope(x,y)\n",
    "    intercept_c = findBestFitIntercept(x,y,slope_m)\n",
    "\n",
    "#   mencari nc yang hilang dan memberikan nilai menggunakan regressi\n",
    "    for i in range(1,(k+2)):\n",
    "        if i not in nc_dict:\n",
    "            nc_dict[i] = (slope_m*i) + intercept_c\n",
    "    \n",
    "    return nc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mencari adjusted count c* di good turing smoothing\n",
    "def findGoodTuringAdjustCount(c, k, nc_dict):\n",
    "   \n",
    "    adjust_count = ( ( (( c + 1)*( nc_dict[c + 1] / nc_dict[c])) - ( c * (k+1) * nc_dict[k+1] / nc_dict[1]) ) /\n",
    "                     ( 1 - (( k + 1)*nc_dict[k + 1] / nc_dict[1]) )\n",
    "                   )\n",
    "    return adjust_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mencari prediksi kalimat menggunakan backoff\n",
    "def doPredictionBackoffGT(input_sen, bi_dict, tri_dict, quad_dict, bi_prob_dict, tri_prob_dict, quad_prob_dict):\n",
    "    token = input_sen.split()\n",
    "    \n",
    "    if input_sen in quad_prob_dict and quad_prob_dict[ input_sen ][0][0]>0:\n",
    "        print(\"quad\")\n",
    "        pred = quad_prob_dict[input_sen][0]\n",
    "    elif ' '.join(token[1:]) in tri_prob_dict and tri_prob_dict[' '.join(token[1:])][0][0]>0:\n",
    "        print(\"tri\")\n",
    "        pred = tri_prob_dict[ ' '.join(token[1:]) ][0]\n",
    "    elif ' '.join(token[2:]) in bi_prob_dict and bi_prob_dict[ ' '.join(token[2:]) ][0][0]>0:\n",
    "        print(\"bi\")\n",
    "        pred = bi_prob_dict[' '.join(token[2:])][0]\n",
    "    else:\n",
    "        print(\"kosong\")\n",
    "        pred = []\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi mencari kemungkiann kata dari kata singkatan\n",
    "def contained(pred,kata):\n",
    "    jumlahs = len(pred)\n",
    "    jumlaht = len(kata)\n",
    "    jumlah = 0\n",
    "    value = 0\n",
    "    i=0\n",
    "    j=0\n",
    "    n = \"\"\n",
    "    veri = 0\n",
    "#   untuk kata yang berulang seperti yaaaaaannnnggg menjadi yang\n",
    "    if len(kata) > 2: \n",
    "        for k in range(0,len(kata)):\n",
    "            if len(kata) > 0:\n",
    "                if k < len(kata)-1:\n",
    "                    if kata[k] != kata[k+1]:\n",
    "                        if value == 1:\n",
    "                            n = n + kata[k] + kata[k]\n",
    "                            veri = 1\n",
    "                            value = 0\n",
    "                        else: \n",
    "                            n = n + kata[k]\n",
    "                            veri = 1\n",
    "                            value = 0\n",
    "                    elif kata[k] == kata[k+1]:\n",
    "                        value+=1\n",
    "                        if k == len(kata)-2:\n",
    "                            if value == 1:\n",
    "                                n = n + kata[k]\n",
    "\n",
    "        if veri != 0:\n",
    "            n = n + kata[len(kata)-1]\n",
    "            kata = n\n",
    "            jumlaht = len(kata)\n",
    "\n",
    "\n",
    "#   menghitung berapa kalimat singkatan yang mengandung kaliamt prediksi\n",
    "    while i < jumlaht:\n",
    "        while j < jumlahs:\n",
    "            if kata[i]==pred[j]:\n",
    "                jumlah+=1\n",
    "                j+=1\n",
    "                break\n",
    "            j+=1\n",
    "        i+=1    \n",
    "        \n",
    "#   mengembalikan nilai true apa bila semua mengandung kata tersebut\n",
    "    if jumlah >= jumlaht:\n",
    "        return True\n",
    "    else :\n",
    "        return False        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk memanggil data dari csv\n",
    "def csvData(namaFile):\n",
    "    path_origin = \"C:\\\\Users\\\\User\\\\Downloads\\\\testcase\\\\\"\n",
    "    with open(path_origin + namaFile, encoding='utf-8-sig', errors='ignore') as f:\n",
    "        count = 0\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                teks1 = re.sub('\\n',' ',row[0])\n",
    "                # menghapus tanda \t\n",
    "                teks2 = re.sub('\t','',teks1)\n",
    "                teks = re.sub('\t','',teks2)\n",
    "                # menghapus angka\n",
    "                result = ''.join(i for i in teks if not i.isdigit())\n",
    "                # menghapus semua karakter ga berguna\n",
    "                clean = re.sub('[-!@#$%^&*)(_+=}{\\|:;\"<>,?/\"“”…•–]',' ',result)\n",
    "                spasi = re.sub(' +', ' ',clean)\n",
    "                # mengecilkan huruf\n",
    "                lower = spasi.lower()\n",
    "                #split = lower.split(\".\")\n",
    "                # kalau menemukan \".\" atau \"\\n\" dijadikan satu array\n",
    "                dataCsv.append(lower)\n",
    "                count = count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat gui\n",
    "\n",
    "# membuat form\n",
    "def makeform(root, field):\n",
    "    print(field)\n",
    "    row = Frame(root)\n",
    "    lab = Label(row, width=22, text=field+\": \", anchor='w')\n",
    "    ent = Text(row, height=5)\n",
    "    row.pack(side=TOP, \n",
    "             fill=X, \n",
    "             padx=5, \n",
    "             pady=5)\n",
    "    lab.pack(side=LEFT)\n",
    "    ent.pack(side=RIGHT, \n",
    "             expand=YES, \n",
    "             fill=X)\n",
    "    entries[field] = ent\n",
    "    return entries\n",
    "\n",
    "# membuat tabel\n",
    "def maketable(root,kataslang,prediksi):\n",
    "    b = Frame(root)\n",
    "    b.pack(side=TOP, \n",
    "             fill=X, \n",
    "             padx=5, \n",
    "             pady=5)\n",
    "    n = len(kataslang)\n",
    "    count = 0\n",
    "    height = 8\n",
    "    width = 2\n",
    "    for i in range(n): #Rows\n",
    "        for j in range(width): #Columns\n",
    "            tab = Entry(b, text=\"\")\n",
    "            if j == 0:\n",
    "                tab.insert(0, kataslang[count])\n",
    "            else:\n",
    "                tab.insert(0, prediksi[count])\n",
    "            tab.grid(row=i, column=j)\n",
    "        count+=1\n",
    "    return tab\n",
    "            \n",
    "def clicked(entries,senInput):\n",
    "    entries['Output'].insert('0.0', senInput[:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Stemming bahasa\n",
    "def fetch(keyword):\n",
    "    rootWord = keyword\n",
    "    pure = keyword\n",
    "    if contains(rootWord) == True:\n",
    "        rootWord = keyword\n",
    "\n",
    "    \n",
    "    elif (rootWord.endswith(\"kan\") or rootWord.endswith(\"kanlah\") or rootWord.endswith(\"kankah\") or rootWord.endswith(\"kanku\") \n",
    "    or rootWord.endswith(\"kanmu\") or rootWord.endswith(\"kannya\") or rootWord.endswith(\"kanpun\") or rootWord.endswith(\"i\") \n",
    "    or rootWord.endswith(\"ilah\") or rootWord.endswith(\"ikah\") or rootWord.endswith(\"iku\") or rootWord.endswith(\"imu\") \n",
    "    or rootWord.endswith(\"inya\") or rootWord.endswith(\"ipun\")):\n",
    "        rootWord = removeSuffix(rootWord)\n",
    "        rootWord = removePrefix(rootWord)\n",
    "        print(rootWord)\n",
    "        if contains(rootWord) == None:\n",
    "            rootWord = keyword;\n",
    "            rootWord = removeSuffiks(rootWord)\n",
    "            rootWord = removePrefix(rootWord)\n",
    "            print(rootWord)\n",
    "            if contains(rootWord) == None:\n",
    "                rootWord = keyword;\n",
    "                rootWord = removePrefix(rootWord)\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        rootWord = removeSuffix(rootWord)\n",
    "        rootWord = removePrefix(rootWord)\n",
    "        if contains(rootWord) == None:\n",
    "            rootWord = keyword;\n",
    "            rootWord = removePrefix(rootWord)\n",
    "        \n",
    "    if contains(rootWord) == None:\n",
    "        return pure;\n",
    "    else:\n",
    "        return rootWord;\n",
    "\n",
    "def removeSuffiks(keyword):\n",
    "    if contains(keyword) == None:\n",
    "        keyword = removePossesiveSuffiks(keyword)\n",
    "        \n",
    "    if contains(keyword) == None:\n",
    "        keyword = removeDerivationSuffiks(keyword)\n",
    "        \n",
    "    return keyword;    \n",
    "\n",
    "def removePossesiveSuffiks(keyword):\n",
    "    if keyword.endswith(\"lah\"):\n",
    "        keyword = keyword[:-3]\n",
    "    elif keyword.endswith(\"kah\"):\n",
    "        keyword = keyword[:-3]\n",
    "    elif keyword.endswith(\"pun\"):\n",
    "        keyword = keyword[:-3]\n",
    "        \n",
    "    if keyword.endswith(\"ku\"):\n",
    "        keyword = keyword[:-2]\n",
    "    elif keyword.endswith(\"mu\"):\n",
    "        keyword = keyword[:-2]\n",
    "    elif keyword.endswith(\"nya\"):\n",
    "        keyword = keyword[:-3]\n",
    "        \n",
    "    return keyword;   \n",
    "\n",
    "def removeDerivationSuffiks(keyword):\n",
    "    if keyword.endswith(\"kan\"):\n",
    "        keyword = keyword[:-3]\n",
    "        \n",
    "    return keyword;     \n",
    "\n",
    "def removeSuffix(keyword):\n",
    "    return removeSuffixes(keyword);\n",
    "\n",
    "def removeSuffixes(keyword):\n",
    "    if contains(keyword) == None:\n",
    "        keyword = removePossesive(keyword)\n",
    "        \n",
    "    if contains(keyword) == None:\n",
    "        keyword = removeDerivationSuffix(keyword)\n",
    "        \n",
    "    return keyword;     \n",
    "\n",
    "def removePossesive(keyword):\n",
    "    if keyword.endswith(\"lah\"):\n",
    "        keyword = keyword[:-3]\n",
    "    elif keyword.endswith(\"kah\"):\n",
    "        keyword = keyword[:-3]\n",
    "    elif keyword.endswith(\"pun\"):\n",
    "        keyword = keyword[:-3]\n",
    "        \n",
    "    if keyword.endswith(\"ku\"):\n",
    "        keyword = keyword[:-2]\n",
    "    elif keyword.endswith(\"mu\"):\n",
    "        keyword = keyword[:-2]\n",
    "    elif keyword.endswith(\"nya\"):\n",
    "        keyword = keyword[:-3]\n",
    "        \n",
    "    return keyword;      \n",
    "\n",
    "def removeDerivationSuffix(keyword):\n",
    "    if keyword.endswith(\"i\"):\n",
    "        keyword = keyword[:-1]\n",
    "    elif keyword.endswith(\"an\"):\n",
    "        keyword = keyword[:-2]\n",
    "        \n",
    "    return keyword\n",
    "    \n",
    "def removePrefix(keyword):\n",
    "    return removeDerivation(keyword)\n",
    "\n",
    "def removeDerivation(keyword):\n",
    "    if contains(keyword) == None:\n",
    "        if keyword.startswith(\"di\"):\n",
    "            keyword = keyword[2:]\n",
    "            if contains(keyword) == None and (keyword.startswith(\"ke\") or keyword.startswith(\"se\")):\n",
    "                keyword = keyword[2:]\n",
    "                \n",
    "        elif keyword.startswith(\"ke\"):\n",
    "            keyword = keyword[2:]\n",
    "            if contains(keyword) == None and (keyword.startswith(\"se\") or keyword.startswith(\"di\")):\n",
    "                keyword = keyword[2:]\n",
    "            \n",
    "        elif keyword.startswith(\"se\"):\n",
    "            keyword = keyword[2:]\n",
    "            if contains(keyword) == None and (keyword.startswith(\"ke\") or keyword.startswith(\"di\")):\n",
    "                keyword = keyword[2:]\n",
    "            \n",
    "    if contains(keyword) == None:\n",
    "        if keyword.startswith(\"me\"):\n",
    "            if re.search(\"[lrwy]\",keyword[2:3]) and re.search(\"[aiueo]\",keyword[3:4]):\n",
    "                keyword = keyword[2:]\n",
    "            \n",
    "            elif keyword.startswith(\"meter\") and len(keyword)>4:\n",
    "                if re.search(\"[aiueo]\",keyword[5:6]) and contains(\"r\"+keyword[5]):\n",
    "                    keyword = \"r\" + keyword[5:]\n",
    "                    \n",
    "                elif re.search(\"[aiueo]\",keyword[5:6]):\n",
    "                    keyword = keyword[5:]\n",
    "                    \n",
    "                elif re.search(\"[aiueor]\",keyword[5:6]) == None and re.search(\"er\",keyword[5:6]) == None:\n",
    "                    keyword = keyword[5:]\n",
    "                    \n",
    "                elif len(keyword)>7:\n",
    "                    if re.search(\"[aiueor]\",keyword[5:6]) == None and re.search(\"er\",keyword[6:8]):\n",
    "                        keyword = keyword[5:]\n",
    "                \n",
    "        \n",
    "            elif keyword.startswith(\"mete\"):\n",
    "                if (len(keyword)>7):\n",
    "                    if re.search(\"[aiueor]\",keyword[4:5]) == None and re.search(\"er\",keyword[5:7]) and re.search(\"[aiueo]\",keyword[7:8]) == None:\n",
    "                        keyword = keyword.substring(4);            \n",
    "            \n",
    "            elif keyword.startswith(\"meng\"):\n",
    "                if re.search(\"[ghq]\",keyword[4:5]):\n",
    "                    keyword = keyword[4:]\n",
    "                elif re.search(\"[aiueo]\",keyword[4:5]) and contains(keyword[4:]):\n",
    "                    keyword = keyword[4:]\n",
    "                elif re.search(\"[aiueo]\",keyword[4:5]) and contains(\"k\"+keyword[4:]):\n",
    "                    keyword = \"k\"+keyword[4:]\n",
    "                    \n",
    "            elif keyword.startswith(\"menter\") and len(keyword)>4:\n",
    "                    \n",
    "                if keyword.substring(6, 7).matches(\"[aiueo]\") and contains(\"r\"+keyword[6:]):\n",
    "                    keyword = \"r\"+keyword[6:]\n",
    "                elif keyword.substring(6, 7).matches(\"[aiueo]\"):\n",
    "                    keyword = keyword[6:]\n",
    "                elif keyword.substring(6, 7).matches(\"[aiueor]\") == None and keyword.substring(6, 7).matches(\"er\") == None:\n",
    "                    keyword = keyword[6:]\n",
    "                elif (len(keyword)>8):\n",
    "                    if keyword.substring(6, 7).matches(\"[aiueor]\") == None and keyword.substring(7, 9).matches(\"er\"):\n",
    "                        keyword = keyword[6:]\n",
    "                \n",
    "            elif keyword.startswith(\"mente\"):\n",
    "                if (len(keyword)>8):\n",
    "                    if keyword.substring(5, 6).matches(\"[aiueor]\") == None and keyword.substring(6, 8).matches(\"er\") and keyword.substring(8, 9).matches(\"[aiueo]\") == None:\n",
    "                        keyword = keyword[5:]\n",
    "    \n",
    "            elif keyword.startswith(\"meny\"):\n",
    "                if contains(\"s\" + keyword[4:]):\n",
    "                    keyword = \"s\" + keyword[4:]\n",
    "            \n",
    "            elif keyword.startswith(\"mempe\") and contains(keyword[4:]):\n",
    "                keyword = keyword[3:]\n",
    "        \n",
    "            elif keyword.startswith(\"mempe\") and len(keyword)>9:\n",
    "                if keyword == \"mempelajari\":\n",
    "                    keyword = \"ajar\"\n",
    "                    \n",
    "                elif keyword.substring(5, 6).matches(\"[l]\") and keyword.substring(6, 7).matches(\"[aiueo]\"):\n",
    "                    keyword = keyword[5:]   \n",
    "                    \n",
    "                elif keyword.substring(5, 6).matches(\"[wy]\"):\n",
    "                    keyword = keyword[5:]            \n",
    "                    \n",
    "                elif keyword.substring(5, 6).matches(\"[rwylmn]\") == None and keyword.substring(6, 8).matches(\"er\") and keyword.substring(8, 9).matches(\"[aiueo]\"):\n",
    "                    keyword = keyword[5:]             \n",
    "\n",
    "                elif keyword.substring(5, 6).matches(\"[rwylmn]\") == None and keyword.substring(6, 8).matches(\"er\") == None:\n",
    "                    keyword = keyword[5:]             \n",
    "\n",
    "                elif keyword.startswith(\"memper\"):\n",
    "                    if keyword.substring(6, 7).matches(\"[aiueo]\") and contains(\"r\" + keyword[6:]):\n",
    "                        keyword = \"r\" + keyword[3:]\n",
    "                        \n",
    "                    elif keyword.substring(6, 7).matches(\"[aiueo]\"):\n",
    "                        keyword = keyword[6:]\n",
    "                        \n",
    "                    elif keyword.substring(6, 7).matches(\"[aiueor]\") == None and keyword.substring(8, 10).matches(\"er\") and keyword.substring(10, 11).matches(\"[aiueo]\"):\n",
    "                        keyword = keyword[3:]                  \n",
    "             \n",
    "                    elif keyword.substring(6, 7).matches(\"[aiueor]\") == None and keyword.substring(8, 10).matches(\"er\") == None:\n",
    "                        keyword = keyword[6:]                  \n",
    "             \n",
    "                elif keyword.startswith(\"mempem\"):\n",
    "                    if keyword.substring(6, 7).matches(\"[bfv]\") and contains(keyword[6:]):\n",
    "                        keyword = keyword[6:]\n",
    "                        \n",
    "                    elif keyword.substring(6, 7).matches(\"[r]\") and keyword.substring(7, 8).matches(\"[aiueo]\") and contains(\"m\" + keyword[6]):\n",
    "                        keyword = \"m\" + keyword[6:]\n",
    "                        \n",
    "                    elif keyword.substring(6, 7).matches(\"[r]\") and keyword.substring(7, 8).matches(\"[aiueo]\") and contains(\"p\" + keyword[6:]):\n",
    "                        keyword = \"p\" + keyword[6:]\n",
    "\n",
    "                    elif keyword.substring(6, 7).matches(\"[aiueo]\") and contains(\"m\" + keyword[6:]):\n",
    "                        keyword = \"m\" + keyword[6:]\n",
    "                        \n",
    "                    elif keyword.substring(6, 7).matches(\"[aiueo]\") and contains(\"p\" + keyword[6:]):\n",
    "                        keyword = \"p\" + keyword[6:]\n",
    "                        \n",
    "                elif keyword.startswith(\"mempeny\"):\n",
    "                    if contains(\"s\" + keyword[7:]):\n",
    "                        keyword = \"s\" + keyword[7:]\n",
    "\n",
    "                elif keyword.startswith(\"mempeng\"):\n",
    "                    if keyword.substring(7, 8).matches(\"[ghq]\"):\n",
    "                        keyword = keyword[7:]\n",
    "                    elif keyword.substring(7, 8).matches(\"[aiueo]\"):\n",
    "                        keyword = keyword[7:]\n",
    "                    elif keyword.substring(7, 8).matches(\"[aiueo]\") and contains(\"k\" + keyword[7:]):\n",
    "                        keyword = \"k\" + keyword[7:]\n",
    "                        \n",
    "                elif keyword.startswith(\"mempen\"):\n",
    "                    if keyword.substring(6, 7).matches(\"[aiueo]\") and contains(\"t\" + keyword[6:]):\n",
    "                        keyword = \"t\" + keyword[6:]                  \n",
    "                    elif keyword.substring(6, 7).matches(\"[aiueo]\") and contains(\"n\" + keyword[6:]):\n",
    "                        keyword = \"n\" + keyword[6:]\n",
    "                    elif keyword.substring(6, 7).matches(\"[jdcz]\"):\n",
    "                        keyword = keyword[6:]\n",
    "                \n",
    "            elif keyword.startswith(\"memp\") and re.search(\"[aiueo]\",keyword[5:6]) and contains(\"p\"+keyword[4:]):\n",
    "                keyword = \"p\"+keyword[4:]\n",
    "            elif keyword.startswith(\"member\") and len(keyword)>6:\n",
    "                if re.search(\"[aiueor]\",keyword[6:7]) == None and re.search(\"er\",keyword[8:10]) and re.search(\"[aiueo]\",keyword[10:11]):\n",
    "                    keyword = keyword[6:] \n",
    "                \n",
    "                elif re.search(\"[aiueor]\",keyword[6:7]) == None and re.search(\"er\",keyword[8:10] == None):\n",
    "                    keyword = keyword[6:] \n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[6:7]) and contains(\"r\"+keyword[6:]):\n",
    "                    keyword = \"r\"+keyword[6:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[6:7]):\n",
    "                    keyword = keyword[6:]\n",
    "               \n",
    "            elif keyword.startswith(\"mem\"):\n",
    "                \n",
    "                if re.search(\"[bvf]\",keyword[3:4]):\n",
    "                    keyword = keyword[3:]\n",
    "                \n",
    "                elif re.search(\"[r]\",keyword[3:4]) and re.search(\"[aiueo]\",keyword[4:5]) and contains(\"m\"+keyword[3:]): \n",
    "                    keyword = \"m\"+keyword[4:]\n",
    "                \n",
    "                elif re.search(\"[r]\",keyword[3:4]) and re.search(\"[aiueo]\",keyword[4:5]) and contains(\"p\"+keyword[3:]): \n",
    "                    keyword = \"p\"+keyword[4:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]) and contains(\"m\"+keyword[3:]): \n",
    "                    keyword = \"m\"+keyword[3:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]) and contains(\"p\"+keyword[3:]): \n",
    "                    keyword = \"p\"+keyword[3:]\n",
    "                \n",
    "            elif keyword.startswith(\"menter\"):\n",
    "                \n",
    "                if re.search(\"[aiueo]\",keyword[6:7]) and contains(\"r\"+keyword[5:]):\n",
    "                    keyword = \"r\"+keyword[5:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[6:7]):\n",
    "                    keyword = keyword[6:]\n",
    "                \n",
    "                elif re.search(\"[aiueor]\",keyword[6:7]) == None and re.search(\"er\",keyword[7:9]) and re.search(\"[aiueo]\",keyword[9:10]):\n",
    "                    keyword = keyword[6:]\n",
    "                \n",
    "                elif re.search(\"[aiueor]\",keyword[6:7]) == None and re.search(\"er\",keyword[6:8]) == None:\n",
    "                    keyword = keyword[6:]\n",
    "                \n",
    "            elif keyword.startswith(\"men\"):\n",
    "                \n",
    "                if re.search(\"[cdjz]\",keyword[3:4]):\n",
    "                    keyword = keyword[3:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]) and contains(\"n\"+keyword[3:]):\n",
    "                    keyword = \"n\"+keyword[3:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]) and contains(\"t\"+keyword[3:]):\n",
    "                    keyword = \"t\"+keyword[3:]\n",
    "                \n",
    "        elif keyword.startswith(\"te\"):\n",
    "            print(\"masuk 01\")\n",
    "            if keyword.startswith(\"ter\"):    \n",
    "                if re.search(\"[aiueo]\",keyword[3:4]) and contains(\"r\"+keyword[3:]):\n",
    "                    keyword = \"r\"+keyword[3:]\n",
    "\n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]):\n",
    "                    keyword = keyword[3:]\n",
    "\n",
    "                elif re.search(\"[aiueor]\",keyword[3:4]) == None and re.search(\"er\",keyword[3:5]) == None:\n",
    "                    keyword = keyword[3:]\n",
    "\n",
    "                elif len(keyword)>5:\n",
    "                    if re.search(\"[aiueor]\",keyword[3:4]) == None and re.search(\"er\",keyword[4:6]):\n",
    "                        keyword = keyword[3:]\n",
    "                \n",
    "            elif keyword.startswith(\"te\"):\n",
    "                if len(keyword)>2:\n",
    "                    if re.search(\"[aiueor]\",keyword[2:3]) == None and re.search(\"er\",keyword[3:5]) and re.search(\"[aiueo]\",keyword[5:6]) == None:\n",
    "                            keyword = keyword[2:]\n",
    "            \n",
    "        elif keyword.startswith(\"be\"):\n",
    "            if keyword == \"belajar\":\n",
    "                keyword = \"ajar\"\n",
    "                \n",
    "            elif re.search(\"[aiueolr]\",keyword[2:3]) == None and re.search(\"er\",keyword[3:5]) and  re.search(\"[aiueor]\",keyword[5:6]) == None:\n",
    "                keyword = keyword[2:]\n",
    "                \n",
    "            elif keyword.startswith(\"berke\") and contains(keyword[5:]):\n",
    "                keyword = keyword[5:]\n",
    "                \n",
    "            elif keyword.startswith(\"berke\") and contains(keyword[3:]):\n",
    "                keyword = keyword[3:]\n",
    "                \n",
    "            elif keyword.startswith(\"ber\"):\n",
    "                \n",
    "                if re.search(\"[aiueo]\",keyword[3:4]) and contains(\"r\"+keyword[3:]):\n",
    "                    keyword = \"r\"+keyword[3:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]):\n",
    "                    keyword = keyword[3:]\n",
    "                \n",
    "                elif len(keyword)>6:\n",
    "                    if re.search(\"[aiueor]\",keyword[3:4]) == None and re.search(\"er\",keyword[5:7]) == None:\n",
    "                        keyword = keyword[3:]\n",
    "                \n",
    "                    elif re.search(\"[aiueor]\",keyword[3:4]) == None and re.search(\"er\",keyword[5:7]) and re.search(\"[aiueo]\",keyword[7:8]):\n",
    "                        keyword = keyword[3:]\n",
    "                   \n",
    "        elif keyword.startswith(\"pe\"):\n",
    "            \n",
    "            if keyword == \"pelajar\":\n",
    "                keyword = \"ajar\"\n",
    "                \n",
    "            elif re.search(\"[l]\",keyword[2:3]) and re.search(\"[aiueo]\",keyword[3:4]):\n",
    "                keyword = keyword[2:]\n",
    "                 \n",
    "            elif re.search(\"[wy]\",keyword[2:3]):\n",
    "                keyword = keyword[2:]\n",
    "                \n",
    "            elif re.search(\"[rwylmn]\",keyword[2:3]) == None and re.search(\"er\",keyword[3:5]) and re.search(\"[aiueo]\",keywordyword[5:6]):\n",
    "                keyword = keyword[2:]\n",
    "             \n",
    "            elif re.search(\"[rwylmn]\",keyword[2:3]) == None and re.search(\"er\",keyword[3:5])== None:\n",
    "                keyword = keyword[2:]\n",
    "             \n",
    "            elif keyword.startswith(\"per\"):\n",
    "                print(\"per\")\n",
    "                if re.search(\"[aiueo]\",keyword[3:4]) and contains(\"r\" + keyword[3:]):\n",
    "                    keyword = \"r\" + keyword[3:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]):\n",
    "                    keyword = keyword[3:]\n",
    "                \n",
    "                elif re.search(\"[aiueor]\",keyword[3:4]) == None and re.search(\"er\",keyword[5:7]) and re.search(\"[aiueo]\",keyword[7:8]):\n",
    "                    keyword = keyword[3:]\n",
    "                    \n",
    "                elif re.search(\"[aiueor]\",keyword[3:4]) == None and re.search(\"er\",keyword[5:7]) == None:\n",
    "                    keyword = keyword[3:]\n",
    "                \n",
    "            elif keyword.startswith(\"pem\"):\n",
    "                \n",
    "                if re.search(\"[bfv]\",keyword[3:4]) and contains(keyword[3:]):\n",
    "                    keyword = keyword[3:]\n",
    "                    \n",
    "                elif re.search(\"[r]\",keyword[3:4]) and re.search(\"[aiueo]\",keyword[4:5]) and contains(\"m\" + keyword[3:]):\n",
    "                    keyword = \"m\" + keyword[3:]\n",
    "                                                                                                 \n",
    "                elif re.search(\"[r]\",keyword[3:4]) and re.search(\"[aiueo]\",keyword[4:5]) and contains(\"p\" + keyword[3:]):     \n",
    "                    keyword = \"p\" + keyword[3:]\n",
    "                                                                                                           \n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]) and contains(\"m\" + keyword[3:]):\n",
    "                    keyword = \"m\" + keyword[3:]\n",
    "                    \n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]) and contains(\"p\" + keyword[3:]):\n",
    "                    keyword = \"p\" + keyword[3:]\n",
    "                                                                                                      \n",
    "                elif re.search(\"[aiueo]\",keyword[6:7]) and contains(\"r\" + keyword[6:]):\n",
    "                    keyword = \"r\" + keyword[6:]\n",
    "                \n",
    "            elif keyword.startswith(\"peny\"):\n",
    "                if contains(\"s\" + keyword[4:]):\n",
    "                    keyword = \"s\" + keyword[4:]\n",
    "                            \n",
    "            elif keyword.startswith(\"peng\"):\n",
    "                            \n",
    "                if re.search(\"[ghq]\",keyword[4:5]):\n",
    "                    keyword = keyword[4:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[4:5]):\n",
    "                    keyword = keyword[4:]\n",
    "                            \n",
    "                elif re.search(\"[aiueo]\",keyword[4:5]) and contains(\"k\" + keyword[4:]):\n",
    "                    keyword = \"k\" + keyword[4:]\n",
    "                    \n",
    "            elif keyword.startswith(\"penter\") and len(keyword)>4:\n",
    "                if re.search(\"[aiueo]\",keyword[6:7]) and contains(\"r\" + keyword[6:]):\n",
    "                    keyword = \"r\" + keyword[6:]\n",
    "                elif re.search(\"[aiueo]\",keyword[6:7]):\n",
    "                    keyword = keyword[6:]\n",
    "                elif re.search(\"[aiueor]\",keyword[6:7]) == None and re.search(\"er\",keyword[6:7]) == None :\n",
    "                    keyword = keyword[6:]\n",
    "                elif len(keyword) > 8:\n",
    "                    if re.search(\"[aiueor]\",keyword[6:7]) == None and re.search(\"er\",keyword[7:9]):\n",
    "                        keyword = keyword[6:]\n",
    "                        \n",
    "            elif keyword.startswith(\"pente\"):\n",
    "                if len(keyword)>8:\n",
    "                     if re.search(\"[aiueor]\",keyword[5:6]) == None and re.search(\"er\",keyword[6:8]) and re.search(\"[aiueo]\",keyword[8:9]) == None:\n",
    "                        keyword = keyword[5:] \n",
    "                        \n",
    "            elif keyword.startswith(\"pen\"):\n",
    "                if re.search(\"[aiueo]\",keyword[3:4]) and contains(\"t\" + keyword[3:]):\n",
    "                    keyword = \"t\" + keyword[3:]\n",
    "\n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]) and contains(\"n\" + keyword[3:]):\n",
    "                    keyword = \"n\" + keyword[3:]\n",
    "                    \n",
    "                elif re.search(\"[jdcz]\",keyword[3:4]):\n",
    "                    keyword = keyword[3:]\n",
    "                    \n",
    "            elif keyword.startswith(\"pen\"):\n",
    "                \n",
    "                if re.search(\"[aiueo]\",keyword[3:4]) and contains(\"t\" + keyword[3:]):\n",
    "                    keyword = \"t\" + keyword[3:]\n",
    "                \n",
    "                elif re.search(\"[aiueo]\",keyword[3:4]) and contains(\"n\" + keyword[3:]):\n",
    "                    keyword = \"n\" + keyword[3:]\n",
    "                \n",
    "                elif re.search(\"[jdcz]\",keyword[3:4]):\n",
    "                    keyword = keyword[3:]\n",
    "            \n",
    "    return keyword\n",
    "\n",
    "\n",
    "# mengecek kata ada di korpus kata atau tidak\n",
    "def contains(teks):\n",
    "    database = \"C:\\\\db\\\\korpus.db\"\n",
    "    conn = create_connection(database)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('Select kata from korpus_kata where kata = (?) and tag != \"Entri tidak ditemukan\" and tag != \"tidak ditemukan kata yang dicari\" and tag != \"entri tidak ditemukan\" ',(teks,))\n",
    "    a = cur.fetchone()\n",
    "    conn.commit()\n",
    "    if a==None:\n",
    "        return None\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(entries,root):\n",
    "    sleng_num = 0  \n",
    "    count_char = 0\n",
    "    max_char = 0\n",
    "#   gui input\n",
    "    input_sen = entries['Input'].get('1.0', END)\n",
    "    print(input_sen) \n",
    "#   memanggil fungsi cekWrong\n",
    "    with conn:\n",
    "        cekWrong(conn,input_sen)  \n",
    "    senInput = input_sen.split()\n",
    "    print(senInput)\n",
    "\n",
    "    true = []\n",
    "    false = []\n",
    "\n",
    "    for sleng in wrong:\n",
    "        max_char = 0\n",
    "        print(wrong)\n",
    "        kalimatTeks(sleng_num)\n",
    "        input_sen = input_teks[0]\n",
    "\n",
    "        token = input_sen.split()\n",
    "        print(input_teks)\n",
    "        print(token)\n",
    "        kataslang.append(kata[wrong[sleng_num]])\n",
    "        error = ' '.join(token[2:])\n",
    "        if error not in bi_prob_dict:\n",
    "            n=0\n",
    "        else:\n",
    "            n = len(bi_prob_dict[' '.join(token[2:])])\n",
    "        print(n)\n",
    "#       menampilkan semua hasil prediksi\n",
    "        for i in range(n):\n",
    "            prob_bigram_char = 1\n",
    "#           memanggil bi_prob_dict\n",
    "            pred = bi_prob_dict[' '.join(token[2:])][i]\n",
    "\n",
    "#           jika kalimat prediksi mengandung kalimat singkatan\n",
    "            if(contained(pred[1],kata[wrong[sleng_num]])==True):\n",
    "                b = pred[1]\n",
    "#               memecah menjadi karakter\n",
    "                bigramchar = [b[j:j+2] for j in range(len(b)-1)]\n",
    "                for x in range(len(bigramchar)):\n",
    "#                   menghitung probabilitas\n",
    "                    prob_bigram_char = prob_bigram_char * bi_prob_dict_char[bigramchar[x]]\n",
    "\n",
    "                print(pred[1] ,\"%.8f\" % prob_bigram_char)\n",
    "#               menghitung probabilitas terbesar\n",
    "                if prob_bigram_char > max_char:\n",
    "                    max_char = prob_bigram_char\n",
    "                    true.clear()\n",
    "                    true.append(pred[1])\n",
    "\n",
    "                if(count_char > 5):\n",
    "                    break\n",
    "                count_char = count_char + 1\n",
    "            list_pred.append(pred[1])\n",
    "\n",
    "#       jika hasil prediksi tidak ada maka mengembalikan kata awal\n",
    "        if not true:\n",
    "            true.append(kata[wrong[sleng_num]])\n",
    "            prediksi.append(kata[wrong[sleng_num]])\n",
    "            f = kata[wrong[sleng_num]]\n",
    "\n",
    "\n",
    "        senInput.pop(sleng)\n",
    "#       mengganti kalimat awal menjadi kalimat prediksi\n",
    "        senInput.insert(sleng,true[0])\n",
    "        print(\" \".join(senInput[:]))\n",
    "\n",
    "        input_teks.clear()\n",
    "        kata.pop(sleng)\n",
    "        kata.insert(sleng,true[0])\n",
    "        prediksi.append(true[0])\n",
    "\n",
    "        sleng_num = sleng_num + 1\n",
    "        true.clear()\n",
    "\n",
    "    print(senInput[:])\n",
    "    entries['Output'].delete('0.0', END)\n",
    "    entries['Output'].insert('0.0', senInput[:])\n",
    "    print(len(prediksi))\n",
    "    print(prediksi)\n",
    "    print(len(kata))\n",
    "    print(kata)\n",
    "    print(len(wrong))\n",
    "    \n",
    "    maketable(root,kataslang,prediksi)\n",
    "    print(\"asu\")\n",
    "    kata.clear()\n",
    "    wrong.clear()\n",
    "    \n",
    "    kataslang.clear()\n",
    "    prediksi.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Kecilkorpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0572aef40fee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#   menjalankan fungsi loadcorpus dan loadcorpuschar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtoken_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbi_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtri_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquad_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mtoken_len_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadCorpusChar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbi_dict_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtri_dict_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquad_dict_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_dict_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2bd9042985cf>\u001b[0m in \u001b[0;36mloadCorpus\u001b[1;34m(file_path, bi_dict, tri_dict, quad_dict, vocab_dict)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#   memuat korpus kalimat dan di baca line per line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Kecilkorpus.txt'"
     ]
    }
   ],
   "source": [
    "token = []\n",
    "kata = []\n",
    "index = []\n",
    "wrong = []\n",
    "input_teks = []\n",
    "test = []\n",
    "true = []\n",
    "prediksi = []\n",
    "kataslang = []\n",
    "dataCsv = []\n",
    "senInput = []\n",
    "entries = {}\n",
    "from tkinter import *\n",
    "colomn_wrong = []\n",
    "colomn_pred = []\n",
    "colomn_prob = []\n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    vocab_dict = defaultdict(int)          #untuk membuat kalimat yang unik menjadi ada frekuensi   \n",
    "    bi_dict = defaultdict(int)             #untuk menyimpan 2 kalimat \n",
    "    tri_dict = defaultdict(int)            #untuk menyimpan 3 kalimat \n",
    "    quad_dict = defaultdict(int)           #untuk menyimpan 4 kalimat \n",
    "    \n",
    "    vocab_dict_char = defaultdict(int)          \n",
    "    bi_dict_char = defaultdict(int)             \n",
    "    tri_dict_char = defaultdict(int)           \n",
    "    quad_dict_char = defaultdict(int)           \n",
    "    \n",
    "    quad_prob_dict = OrderedDict()              \n",
    "    tri_prob_dict = OrderedDict()\n",
    "    bi_prob_dict = OrderedDict()\n",
    "    \n",
    "    quad_prob_dict_char = OrderedDict()              \n",
    "    tri_prob_dict_char = OrderedDict()\n",
    "    bi_prob_dict_char = OrderedDict()\n",
    "    \n",
    "    \n",
    "    list_pred = []\n",
    "#   koneksi ke db\n",
    "    database = \"C:\\\\db\\\\korpus.db\"\n",
    "    conn = create_connection(database)\n",
    "    \n",
    "#   membuka korpus kecil\n",
    "    train_file = 'Kecilkorpus.txt'\n",
    "\n",
    "#   menjalankan fungsi loadcorpus dan loadcorpuschar\n",
    "    token_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "    token_len_char = loadCorpusChar(train_file, bi_dict_char, tri_dict_char, quad_dict_char, vocab_dict_char)\n",
    "\n",
    "\n",
    "    k = 5\n",
    "    V = len(vocab_dict)\n",
    "    \n",
    "    tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "    bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "    uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "    \n",
    "    tri_nc_dict_char = findFrequencyOfFrequencyCount(tri_dict_char, k, 3, V, len(tri_dict_char))\n",
    "    bi_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 2, V, len(bi_dict_char))\n",
    "    uni_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 1, V, len(vocab_dict_char))\n",
    "    \n",
    "\n",
    "#   membuat dic probabilitas bigram, trigram, charbigram, chartrigram\n",
    "    findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, bi_nc_dict, k)\n",
    "    findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k)\n",
    "    \n",
    "    findCharBigramProbGT(vocab_dict_char, bi_dict_char, bi_prob_dict_char, bi_nc_dict_char, k)\n",
    "    findCharTrigramProbGT(vocab_dict_char, bi_dict_char, tri_dict_char, tri_prob_dict_char, tri_nc_dict_char, k)\n",
    "    \n",
    "    sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "\n",
    "    \n",
    "    entries = {}\n",
    "    root = Tk()\n",
    "    \n",
    "    ents = makeform(root, 'Input')\n",
    "    \n",
    "    \n",
    "    b1 = Button(root, text='Normalisasi', command=(lambda e=ents: main(e,root)))\n",
    "    b1.pack(side=TOP, fill=X, padx=5, pady=5)\n",
    "    \n",
    "    ents = makeform(root, 'Output') \n",
    "    \n",
    "\n",
    "    root.mainloop()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #variable declaration\n",
    "    \n",
    "    vocab_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "    bi_dict = defaultdict(int)             #for keeping count of sentences of two words\n",
    "    tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "    quad_dict = defaultdict(int)           #for keeping count of sentences of four words\n",
    "    \n",
    "    vocab_dict_char = defaultdict(int)          \n",
    "    bi_dict_char = defaultdict(int)             \n",
    "    tri_dict_char = defaultdict(int)           \n",
    "    quad_dict_char = defaultdict(int)           \n",
    "    \n",
    "    quad_prob_dict = OrderedDict()              \n",
    "    tri_prob_dict = OrderedDict()\n",
    "    bi_prob_dict = OrderedDict()\n",
    "    \n",
    "    quad_prob_dict_char = OrderedDict()              \n",
    "    tri_prob_dict_char = OrderedDict()\n",
    "    bi_prob_dict_char = OrderedDict()\n",
    "    \n",
    "    \n",
    "    list_pred = []\n",
    "    database = \"C:\\\\db\\\\korpus.db\"\n",
    "    conn = create_connection(database)\n",
    "#     with conn:\n",
    "#         cek(conn)\n",
    "#     kalimat()\n",
    "    \n",
    "    #load the corpus for the dataset\n",
    "    train_file = 'BesarKorpus.txt'\n",
    "    #load corpus\n",
    "\n",
    "    token_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "    token_len_char = loadCorpusChar(train_file, bi_dict_char, tri_dict_char, quad_dict_char, vocab_dict_char)\n",
    "\n",
    "    #create the different Nc dictionaries for ngrams\n",
    "    #threshold value\n",
    "    k = 5\n",
    "    V = len(vocab_dict)\n",
    "    \n",
    "    tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "    bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "    uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "    \n",
    "    tri_nc_dict_char = findFrequencyOfFrequencyCount(tri_dict_char, k, 3, V, len(tri_dict_char))\n",
    "    bi_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 2, V, len(bi_dict_char))\n",
    "    uni_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 1, V, len(vocab_dict_char))\n",
    "    \n",
    "\n",
    "    #create bigram probability dictionary\n",
    "    findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, bi_nc_dict, k)\n",
    "    findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k)\n",
    "    \n",
    "    findCharBigramProbGT(vocab_dict_char, bi_dict_char, bi_prob_dict_char, bi_nc_dict_char, k)\n",
    "    findCharTrigramProbGT(vocab_dict_char, bi_dict_char, tri_dict_char, tri_prob_dict_char, tri_nc_dict_char, k)\n",
    "    \n",
    "    sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "    \n",
    "    path_origin = \"C:\\\\Users\\\\User\\\\Downloads\\\\testcase\\\\\"\n",
    "    for file in os.listdir(path_origin):\n",
    "        file = file.split(\"\\\\\")\n",
    "        file_name = file[len(file)-1]\n",
    "\n",
    "        dataCsv.clear()\n",
    "        csvData(file_name)\n",
    "        for data in dataCsv:\n",
    "            sleng_num = 0  \n",
    "            count_char = 0\n",
    "            max_char = 0\n",
    "            \n",
    "            input_sen = data\n",
    "            print(input_sen)  \n",
    "        #         input_sen = data\n",
    "            with conn:\n",
    "                cekWrong(conn,input_sen)  \n",
    "            senInput = input_sen.split()\n",
    "            print(senInput)\n",
    "\n",
    "            true = []\n",
    "            false = []\n",
    "            for sleng in wrong:\n",
    "                max_char = 0\n",
    "                print(wrong)\n",
    "                kalimatTeks(sleng_num)\n",
    "                input_sen = input_teks[0]\n",
    "\n",
    "                token = input_sen.split()\n",
    "                print(input_teks)\n",
    "                print(token)\n",
    "                print(kata[wrong[sleng_num]])\n",
    "                error = ' '.join(token[2:])\n",
    "                if error not in bi_prob_dict:\n",
    "                    n=0\n",
    "                else:\n",
    "                    n = len(bi_prob_dict[' '.join(token[2:])])\n",
    "                print(n)\n",
    "                for i in range(n):\n",
    "                    prob_bigram_char = 1\n",
    "                    pred = bi_prob_dict[' '.join(token[2:])][i]\n",
    "            #         print(pred[1])\n",
    "                    if(contained(pred[1],kata[wrong[sleng_num]])==True):\n",
    "                        b = pred[1]\n",
    "                        bigramchar = [b[j:j+2] for j in range(len(b)-1)]\n",
    "                        for x in range(len(bigramchar)):\n",
    "                            prob_bigram_char = prob_bigram_char * bi_prob_dict_char[bigramchar[x]]\n",
    "\n",
    "                        print(pred[1] ,\"%.8f\" % prob_bigram_char)\n",
    "                        print(max_char)\n",
    "                        if prob_bigram_char > max_char:\n",
    "                            max_char = prob_bigram_char\n",
    "                            true.clear()\n",
    "                            true.append(pred[1])\n",
    "                            benar = pred[1]\n",
    "                        \n",
    "                        if(count_char > 5):\n",
    "                            break\n",
    "                        count_char = count_char + 1\n",
    "        #                 else :\n",
    "        #                     print(pred[1] , \"False\")\n",
    "                    list_pred.append(pred[1])\n",
    "\n",
    "                if not true:\n",
    "                    true.append(kata[wrong[sleng_num]])\n",
    "                    f = kata[wrong[sleng_num]]\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(\" \")\n",
    "                    colomn_prob.append(\" \")\n",
    "                    \n",
    "                else :\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(benar)\n",
    "                    colomn_prob.append(format(max_char,'.9f'))\n",
    "\n",
    "        #             bisa dijadikan fungsi    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                senInput.pop(sleng)\n",
    "                senInput.insert(sleng,true[0])\n",
    "        #         senInput[sleng] = true[0]\n",
    "                print(\" \".join(senInput[:]))\n",
    "\n",
    "                input_teks.clear()\n",
    "                kata.pop(sleng)\n",
    "                kata.insert(sleng,true[0])\n",
    "\n",
    "        #         input_teks.append(\" \".join(senInput[:-1]))\n",
    "                sleng_num = sleng_num + 1\n",
    "                true.clear()\n",
    "\n",
    "            print(senInput[:])\n",
    "            kata.clear()\n",
    "            wrong.clear()\n",
    "            kalimatpred.append(\" \".join(senInput[:]))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "#         test = pd.DataFrame(colomn_wrong)\n",
    "#         test.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\Test Singkatan\\\\KorpusKecilBigram%sWrong.csv\"%(file_name), index = False, header = False)  \n",
    "#         test1 = pd.DataFrame(colomn_prob)\n",
    "#         test1.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\Test Singkatan\\\\KorpusKecilBigram%sProb.csv\"%(file_name), index = False, header = False)  \n",
    "#         test2 = pd.DataFrame(colomn_pred)\n",
    "#         test2.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\Test Singkatan\\\\KorpusKecilBigram%sPred.csv\"%(file_name), index = False, header = False)\n",
    "        test2 = pd.DataFrame(kalimatpred)\n",
    "        test2.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarKalimatBigram%sPred.csv\"%(file_name), index = False, header = False)\n",
    "        kalimatpred.clear()\n",
    "#         print(colomn_wrong)\n",
    "#         print(colomn_pred)\n",
    "#         print(colomn_prob)\n",
    "#         colomn_wrong.clear()\n",
    "#         colomn_pred.clear()\n",
    "#         colomn_prob.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'BesarKorpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-82dc45b6e901>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mcolomn_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-c460c4069312>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m#load corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mtoken_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbi_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtri_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquad_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mtoken_len_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadCorpusChar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbi_dict_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtri_dict_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquad_dict_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_dict_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2bd9042985cf>\u001b[0m in \u001b[0;36mloadCorpus\u001b[1;34m(file_path, bi_dict, tri_dict, quad_dict, vocab_dict)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#   memuat korpus kalimat dan di baca line per line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'BesarKorpus.txt'"
     ]
    }
   ],
   "source": [
    "token = []\n",
    "kata = []\n",
    "index = []\n",
    "wrong = []\n",
    "input_teks = []\n",
    "test = []\n",
    "true = []\n",
    "dataCsv = []\n",
    "\n",
    "kalimatpred = []\n",
    "\n",
    "colomn_wrong = []\n",
    "colomn_pred = []\n",
    "colomn_prob = []\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #variable declaration\n",
    "    \n",
    "    vocab_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "    bi_dict = defaultdict(int)             #for keeping count of sentences of two words\n",
    "    tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "    quad_dict = defaultdict(int)           #for keeping count of sentences of four words\n",
    "    \n",
    "    vocab_dict_char = defaultdict(int)          \n",
    "    bi_dict_char = defaultdict(int)             \n",
    "    tri_dict_char = defaultdict(int)           \n",
    "    quad_dict_char = defaultdict(int)           \n",
    "    \n",
    "    quad_prob_dict = OrderedDict()              \n",
    "    tri_prob_dict = OrderedDict()\n",
    "    bi_prob_dict = OrderedDict()\n",
    "    \n",
    "    quad_prob_dict_char = OrderedDict()              \n",
    "    tri_prob_dict_char = OrderedDict()\n",
    "    bi_prob_dict_char = OrderedDict()\n",
    "    \n",
    "    \n",
    "    list_pred = []\n",
    "    database = \"C:\\\\db\\\\korpus.db\"\n",
    "    conn = create_connection(database)\n",
    "#     with conn:\n",
    "#         cek(conn)\n",
    "#     kalimat()\n",
    "    \n",
    "    #load the corpus for the dataset\n",
    "    train_file = 'BesarKorpus.txt'\n",
    "    #load corpus\n",
    "\n",
    "    token_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "    token_len_char = loadCorpusChar(train_file, bi_dict_char, tri_dict_char, quad_dict_char, vocab_dict_char)\n",
    "\n",
    "    #create the different Nc dictionaries for ngrams\n",
    "    #threshold value\n",
    "    k = 5\n",
    "    V = len(vocab_dict)\n",
    "    \n",
    "    tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "    bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "    uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "    \n",
    "    tri_nc_dict_char = findFrequencyOfFrequencyCount(tri_dict_char, k, 3, V, len(tri_dict_char))\n",
    "    bi_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 2, V, len(bi_dict_char))\n",
    "    uni_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 1, V, len(vocab_dict_char))\n",
    "    \n",
    "\n",
    "    #create bigram probability dictionary\n",
    "    findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, bi_nc_dict, k)\n",
    "    findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k)\n",
    "    \n",
    "    findCharBigramProbGT(vocab_dict_char, bi_dict_char, bi_prob_dict_char, bi_nc_dict_char, k)\n",
    "    findCharTrigramProbGT(vocab_dict_char, bi_dict_char, tri_dict_char, tri_prob_dict_char, tri_nc_dict_char, k)\n",
    "    \n",
    "    sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "    path_origin = \"C:\\\\Users\\\\User\\\\Downloads\\\\testcase\\\\\"\n",
    "    for file in os.listdir(path_origin):\n",
    "        file = file.split(\"\\\\\")\n",
    "        file_name = file[len(file)-1]\n",
    "\n",
    "        dataCsv.clear()\n",
    "        csvData(file_name)\n",
    "        for data in dataCsv:\n",
    "            sleng_num = 0  \n",
    "            count_char = 0\n",
    "            max_char = 0\n",
    "            input_sen = data\n",
    "            print(input_sen) \n",
    "            print(\"input_sen\")\n",
    "        #         input_sen = data\n",
    "            with conn:\n",
    "                cekWrong(conn,input_sen)  \n",
    "            senInput = input_sen.split()\n",
    "            print(senInput)\n",
    "            print(\"senInput\")\n",
    "\n",
    "            true = []\n",
    "            false = []\n",
    "            for sleng in wrong:\n",
    "                max_char = 0\n",
    "                print(wrong)\n",
    "                print(\"wrong\")\n",
    "                kalimatTeks(sleng_num)\n",
    "                input_sen = input_teks[0]\n",
    "\n",
    "                token = input_sen.split()\n",
    "                print(input_teks)\n",
    "                print(\"input_teks\")\n",
    "                print(token)\n",
    "                print(\"token\")\n",
    "                print(kata[wrong[sleng_num]])\n",
    "                print(\"kata wrong\")\n",
    "#                 Bigram\n",
    "#                 error = ' '.join(token[2:])\n",
    "\n",
    "#                 Tigram\n",
    "                error = ' '.join(token[1:])\n",
    "                print(error)\n",
    "                print(\"error\")\n",
    "                if error not in tri_prob_dict:\n",
    "                    n=0\n",
    "                else:\n",
    "                    n = len(tri_prob_dict[' '.join(token[1:])])\n",
    "                print(n)\n",
    "                print(\"panjang n\")\n",
    "                for i in range(n):\n",
    "                    prob_trigram_char = 1\n",
    "                    pred = tri_prob_dict[' '.join(token[1:])][i]\n",
    "            #         print(pred[1])\n",
    "                    if(contained(pred[1],kata[wrong[sleng_num]])==True):\n",
    "                        b = pred[1]\n",
    "                        print(b)\n",
    "                        trigramchar = [b[j:j+3] for j in range(len(b)-2)]\n",
    "                        for x in range(len(trigramchar)):\n",
    "                            if trigramchar[x] not in tri_prob_dict_char:\n",
    "                                prob_trigram_char = prob_trigram_char * tri_prob_dict_char[trigramchar[x]]\n",
    "                            else:\n",
    "                                prob_trigram_char = prob_trigram_char * tri_prob_dict_char[trigramchar[x]]\n",
    "\n",
    "                        print(pred[1] ,\"%.8f\" % prob_trigram_char)\n",
    "                        print(\"pred dan prob\")\n",
    "                        if prob_trigram_char > max_char:\n",
    "                            max_char = prob_trigram_char\n",
    "                            true.clear()\n",
    "                            true.append(pred[1])\n",
    "                            benar = pred[1]\n",
    "\n",
    "                        if(count_char > 5):\n",
    "                            break\n",
    "                        count_char = count_char + 1\n",
    "        #                 else :\n",
    "        #                     print(pred[1] , \"False\")\n",
    "                    list_pred.append(pred[1])\n",
    "\n",
    "                if not true:\n",
    "                    true.append(kata[wrong[sleng_num]])\n",
    "                    f = kata[wrong[sleng_num]]\n",
    "                    colomn_wrong.append(f)\n",
    "                    colomn_pred.append(\" \")\n",
    "                    colomn_prob.append(\" \")\n",
    "                else :\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(benar)\n",
    "                    colomn_prob.append(format(max_char,'.9f'))                  \n",
    "                \n",
    "\n",
    "        #             bisa dijadikan fungsi    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                senInput.pop(sleng)\n",
    "                senInput.insert(sleng,true[0])\n",
    "        #         senInput[sleng] = true[0]\n",
    "                print(\" \".join(senInput[:]))\n",
    "                print(\"Gabungan\")\n",
    "            \n",
    "\n",
    "                input_teks.clear()\n",
    "                kata.pop(sleng)\n",
    "                kata.insert(sleng,true[0])\n",
    "\n",
    "        #         input_teks.append(\" \".join(senInput[:-1]))\n",
    "                sleng_num = sleng_num + 1\n",
    "                true.clear()\n",
    "\n",
    "            print(senInput[:])\n",
    "            print(\"senInput\")\n",
    "            kata.clear()\n",
    "            wrong.clear()\n",
    "            kalimatpred.append(\" \".join(senInput[:]))\n",
    "\n",
    "        test2 = pd.DataFrame(kalimatpred)\n",
    "        test2.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarKalimatTrigram%sPred.csv\"%(file_name), index = False, header = False)\n",
    "        kalimatpred.clear()\n",
    "#         test = pd.DataFrame(colomn_wrong)\n",
    "#         test.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\Test Salah\\\\KorpusBesarTrigram%sWrong.csv\"%(file_name), index = False, header = False)  \n",
    "#         test1 = pd.DataFrame(colomn_prob)\n",
    "#         test1.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\Test salah\\\\KorpusBesarTrigram%sProb.csv\"%(file_name), index = False, header = False)  \n",
    "#         test2 = pd.DataFrame(colomn_pred)\n",
    "#         test2.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\Test salah\\\\KorpusBesarTrigram%sPred.csv\"%(file_name), index = False, header = False)  \n",
    "#         print(colomn_wrong)\n",
    "#         print(colomn_pred)\n",
    "#         print(colomn_prob)\n",
    "#         colomn_wrong.clear()\n",
    "#         colomn_pred.clear()\n",
    "#         colomn_prob.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #variable declaration\n",
    "    \n",
    "    vocab_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "    bi_dict = defaultdict(int)             #for keeping count of sentences of two words\n",
    "    tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "    quad_dict = defaultdict(int)           #for keeping count of sentences of four words\n",
    "    \n",
    "    vocab_dict_char = defaultdict(int)          \n",
    "    bi_dict_char = defaultdict(int)             \n",
    "    tri_dict_char = defaultdict(int)           \n",
    "    quad_dict_char = defaultdict(int)           \n",
    "    \n",
    "    quad_prob_dict = OrderedDict()              \n",
    "    tri_prob_dict = OrderedDict()\n",
    "    bi_prob_dict = OrderedDict()\n",
    "    \n",
    "    quad_prob_dict_char = OrderedDict()              \n",
    "    tri_prob_dict_char = OrderedDict()\n",
    "    bi_prob_dict_char = OrderedDict()\n",
    "    \n",
    "    \n",
    "    list_pred = []\n",
    "    database = \"C:\\\\db\\\\korpus.db\"\n",
    "    conn = create_connection(database)\n",
    "#     with conn:\n",
    "#         cek(conn)\n",
    "#     kalimat()\n",
    "    \n",
    "    #load the corpus for the dataset\n",
    "    train_file = 'BesarKorpus.txt'\n",
    "    #load corpus\n",
    "\n",
    "    token_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "    token_len_char = loadCorpusChar(train_file, bi_dict_char, tri_dict_char, quad_dict_char, vocab_dict_char)\n",
    "\n",
    "    #create the different Nc dictionaries for ngrams\n",
    "    #threshold value\n",
    "    k = 5\n",
    "    V = len(vocab_dict)\n",
    "    \n",
    "    tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "    bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "    uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "    \n",
    "    tri_nc_dict_char = findFrequencyOfFrequencyCount(tri_dict_char, k, 3, V, len(tri_dict_char))\n",
    "    bi_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 2, V, len(bi_dict_char))\n",
    "    uni_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 1, V, len(vocab_dict_char))\n",
    "    \n",
    "\n",
    "    #create bigram probability dictionary\n",
    "    findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, bi_nc_dict, k)\n",
    "    findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k)\n",
    "    \n",
    "    findCharBigramProbGT(vocab_dict_char, bi_dict_char, bi_prob_dict_char, bi_nc_dict_char, k)\n",
    "    findCharTrigramProbGT(vocab_dict_char, bi_dict_char, tri_dict_char, tri_prob_dict_char, tri_nc_dict_char, k)\n",
    "    \n",
    "    sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "    path_origin = \"C:\\\\Users\\\\User\\\\Downloads\\\\testcase\\\\\"\n",
    "    for file in os.listdir(path_origin):\n",
    "        file = file.split(\"\\\\\")\n",
    "        file_name = file[len(file)-1]\n",
    "\n",
    "        dataCsv.clear()\n",
    "        csvData(file_name)\n",
    "        for data in dataCsv:\n",
    "            sleng_num = 0  \n",
    "            count_char = 0\n",
    "            max_char = 0\n",
    "            input_sen = data\n",
    "            print(input_sen) \n",
    "            print(\"input_sen\")\n",
    "        #         input_sen = data\n",
    "            with conn:\n",
    "                cekWrong(conn,input_sen)  \n",
    "            senInput = input_sen.split()\n",
    "            print(senInput)\n",
    "            print(\"senInput\")\n",
    "\n",
    "            true = []\n",
    "            false = []\n",
    "            for sleng in wrong:\n",
    "                max_char = 0\n",
    "                print(wrong)\n",
    "                print(\"wrong\")\n",
    "                kalimatTeks(sleng_num)\n",
    "                input_sen = input_teks[0]\n",
    "\n",
    "                token = input_sen.split()\n",
    "                print(input_teks)\n",
    "                print(\"input_teks\")\n",
    "                print(token)\n",
    "                print(\"token\")\n",
    "                print(kata[wrong[sleng_num]])\n",
    "                print(\"kata wrong\")\n",
    "#                 Bigram\n",
    "#                 error = ' '.join(token[2:])\n",
    "\n",
    "#                 Tigram\n",
    "                error = ' '.join(token[1:])\n",
    "                print(error)\n",
    "                print(\"error\")\n",
    "                if error not in tri_prob_dict:\n",
    "                    n=0\n",
    "                else:\n",
    "                    n = len(tri_prob_dict[' '.join(token[1:])])\n",
    "                print(n)\n",
    "                print(\"panjang n\")\n",
    "                for i in range(n):\n",
    "                    pred = tri_prob_dict[' '.join(token[1:])][i]\n",
    "                    prob_trigram_char = pred[0]\n",
    "            #         print(pred[1])\n",
    "                    if(contained(pred[1],kata[wrong[sleng_num]])==True):\n",
    "                        b = pred[1]\n",
    "                        print(b)\n",
    "\n",
    "                        print(pred[1] ,\"%.8f\" % pred[0])\n",
    "                        print(\"pred dan prob\")\n",
    "                        if prob_trigram_char > max_char:\n",
    "                            max_char = prob_trigram_char\n",
    "                            true.clear()\n",
    "                            true.append(pred[1])\n",
    "                            benar = pred[1]\n",
    "\n",
    "                        if(count_char > 5):\n",
    "                            break\n",
    "                        count_char = count_char + 1\n",
    "        #                 else :\n",
    "        #                     print(pred[1] , \"False\")\n",
    "                    list_pred.append(pred[1])\n",
    "\n",
    "                if not true:\n",
    "                    true.append(kata[wrong[sleng_num]])\n",
    "                    f = kata[wrong[sleng_num]]\n",
    "                    colomn_wrong.append(f)\n",
    "                    colomn_pred.append(\" \")\n",
    "                    colomn_prob.append(\" \")\n",
    "                else :\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(benar)\n",
    "                    colomn_prob.append(format(max_char,'.9f'))                  \n",
    "                \n",
    "\n",
    "        #             bisa dijadikan fungsi    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                senInput.pop(sleng)\n",
    "                senInput.insert(sleng,true[0])\n",
    "        #         senInput[sleng] = true[0]\n",
    "                print(\" \".join(senInput[:]))\n",
    "                print(\"Gabungan\")\n",
    "            \n",
    "\n",
    "                input_teks.clear()\n",
    "                kata.pop(sleng)\n",
    "                kata.insert(sleng,true[0])\n",
    "\n",
    "        #         input_teks.append(\" \".join(senInput[:-1]))\n",
    "                sleng_num = sleng_num + 1\n",
    "                true.clear()\n",
    "\n",
    "            print(senInput[:])\n",
    "            print(\"senInput\")\n",
    "            kata.clear()\n",
    "            wrong.clear()\n",
    "            kalimatpred.append(\" \".join(senInput[:]))\n",
    "\n",
    "\n",
    "\n",
    "        test1 = pd.DataFrame(colomn_prob)\n",
    "        test1.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarTrigram%sProb.csv\"%(file_name), index = False, header = False)  \n",
    "        test2 = pd.DataFrame(colomn_pred)\n",
    "        test2.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarTrigram%sPred.csv\"%(file_name), index = False, header = False)  \n",
    "        test3 = pd.DataFrame(kalimatpred)\n",
    "        test3.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarKalimatTrigramNgramWord%sPred.csv\"%(file_name), index = False, header = False)\n",
    "        kalimatpred.clear()\n",
    "#         print(colomn_wrong)\n",
    "#         print(colomn_pred)\n",
    "#         print(colomn_prob)\n",
    "        colomn_wrong.clear()\n",
    "        colomn_pred.clear()\n",
    "        colomn_prob.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}