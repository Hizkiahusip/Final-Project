{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source Code:\n",
    "def main():\n",
    "    #variable declaration\n",
    "    vocab_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "    bi_dict = defaultdict(int)             #for keeping count of sentences of two words\n",
    "    tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "    quad_dict = defaultdict(int)           #for keeping count of sentences of four words\n",
    "    vocab_dict_char = defaultdict(int)          \n",
    "    bi_dict_char = defaultdict(int)             \n",
    "    tri_dict_char = defaultdict(int)           \n",
    "    quad_dict_char = defaultdict(int)           \n",
    "    quad_prob_dict = OrderedDict()              \n",
    "    tri_prob_dict = OrderedDict()\n",
    "    bi_prob_dict = OrderedDict()\n",
    "    quad_prob_dict_char = OrderedDict()              \n",
    "    tri_prob_dict_char = OrderedDict()\n",
    "    bi_prob_dict_char = OrderedDict()\n",
    "    list_pred = []\n",
    "    database = \"C:\\\\db\\\\korpus.db\"\n",
    "    conn = create_connection(database)\n",
    "#     kalimat()\n",
    "    #load the corpus for the dataset\n",
    "    train_file = 'BesarKorpus.txt'\n",
    "    #load corpus\n",
    "    token_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "    token_len_char = loadCorpusChar(train_file, bi_dict_char, tri_dict_char, quad_dict_char, vocab_dict_char)\n",
    "    #create the different Nc dictionaries for ngrams\n",
    "    #threshold value\n",
    "    k = 5\n",
    "    V = len(vocab_dict)\n",
    "    tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "    bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "    uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "    tri_nc_dict_char = findFrequencyOfFrequencyCount(tri_dict_char, k, 3, V, len(tri_dict_char))\n",
    "    bi_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 2, V, len(bi_dict_char))\n",
    "    uni_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 1, V, len(vocab_dict_char))\n",
    "    #create bigram probability dictionary\n",
    "    findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, bi_nc_dict, k)\n",
    "    findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k)\n",
    "    findCharBigramProbGT(vocab_dict_char, bi_dict_char, bi_prob_dict_char, bi_nc_dict_char, k)\n",
    "    findCharTrigramProbGT(vocab_dict_char, bi_dict_char, tri_dict_char, tri_prob_dict_char, tri_nc_dict_char, k)\n",
    "    sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "    path_origin = \"C:\\\\Users\\\\User\\\\Downloads\\\\testcase\\\\\"\n",
    "    for file in os.listdir(path_origin):\n",
    "        file = file.split(\"\\\\\")\n",
    "        file_name = file[len(file)-1]\n",
    "        dataCsv.clear()\n",
    "        csvData(file_name)\n",
    "        for data in dataCsv:\n",
    "            sleng_num = 0  \n",
    "            count_char = 0\n",
    "            max_char = 0\n",
    "            input_sen = data\n",
    "            print(input_sen)  \n",
    "            with conn:\n",
    "                cekWrong(conn,input_sen)  \n",
    "            senInput = input_sen.split()\n",
    "            print(senInput)\n",
    "            true = []\n",
    "            false = []\n",
    "            for sleng in wrong:\n",
    "                max_char = 0\n",
    "                print(wrong)\n",
    "                kalimatTeks(sleng_num)\n",
    "                input_sen = input_teks[0]\n",
    "                token = input_sen.split()\n",
    "                print(input_teks)\n",
    "                print(token)\n",
    "                print(kata[wrong[sleng_num]])\n",
    "                error = ' '.join(token[2:])\n",
    "                if error not in bi_prob_dict:\n",
    "                    n=0\n",
    "                else:\n",
    "                    n = len(bi_prob_dict[' '.join(token[2:])])\n",
    "                print(n)\n",
    "                for i in range(n):\n",
    "                    prob_bigram_char = 1\n",
    "                    pred = bi_prob_dict[' '.join(token[2:])][i]\n",
    "                    if(contained(pred[1],kata[wrong[sleng_num]])==True):\n",
    "                        b = pred[1]\n",
    "                        bigramchar = [b[j:j+2] for j in range(len(b)-1)]\n",
    "                        for x in range(len(bigramchar)):\n",
    "                            prob_bigram_char = prob_bigram_char * bi_prob_dict_char[bigramchar[x]]\n",
    "                        print(pred[1] ,\"%.8f\" % prob_bigram_char)\n",
    "                        print(max_char)\n",
    "                        if prob_bigram_char > max_char:\n",
    "                            max_char = prob_bigram_char\n",
    "                            true.clear()\n",
    "                            true.append(pred[1])\n",
    "                            benar = pred[1]\n",
    "                        if(count_char > 5):\n",
    "                            break\n",
    "                        count_char = count_char + 1\n",
    "                    list_pred.append(pred[1])\n",
    "                if not true:\n",
    "                    true.append(kata[wrong[sleng_num]])\n",
    "                    f = kata[wrong[sleng_num]]\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(\" \")\n",
    "                    colomn_prob.append(\" \")\n",
    "                else :\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(benar)\n",
    "                    colomn_prob.append(format(max_char,'.9f'))\n",
    "        #             bisa dijadikan fungsi    \n",
    "                senInput.pop(sleng)\n",
    "                senInput.insert(sleng,true[0])\n",
    "                print(\" \".join(senInput[:]))\n",
    "                input_teks.clear()\n",
    "                kata.pop(sleng)\n",
    "                kata.insert(sleng,true[0])\n",
    "                sleng_num = sleng_num + 1\n",
    "                true.clear()\n",
    "            print(senInput[:])\n",
    "            kata.clear()\n",
    "            wrong.clear()\n",
    "            kalimatpred.append(\" \".join(senInput[:]))\n",
    "      test2.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarKalimatBigram%sPred.csv\"%(file_name), index = False, header = False)\n",
    "        kalimatpred.clear()\n",
    "token = []\n",
    "kata = []\n",
    "index = []\n",
    "wrong = []\n",
    "input_teks = []\n",
    "test = []\n",
    "true = []\n",
    "dataCsv = []\n",
    "kalimatpred = []\n",
    "colomn_wrong = []\n",
    "colomn_pred = []\n",
    "colomn_prob = []\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "def main():\n",
    "    #variable declaration\n",
    "    vocab_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "    bi_dict = defaultdict(int)             #for keeping count of sentences of two words\n",
    "    tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "    quad_dict = defaultdict(int)           #for keeping count of sentences of four words\n",
    "    vocab_dict_char = defaultdict(int)          \n",
    "    bi_dict_char = defaultdict(int)             \n",
    "    tri_dict_char = defaultdict(int)           \n",
    "    quad_dict_char = defaultdict(int)           \n",
    "    quad_prob_dict = OrderedDict()              \n",
    "    tri_prob_dict = OrderedDict()\n",
    "    bi_prob_dict = OrderedDict()\n",
    "    quad_prob_dict_char = OrderedDict()              \n",
    "    tri_prob_dict_char = OrderedDict()\n",
    "    bi_prob_dict_char = OrderedDict()\n",
    "    list_pred = []\n",
    "    database = \"C:\\\\db\\\\korpus.db\"\n",
    "    conn = create_connection(database)    \n",
    "    #load the corpus for the dataset\n",
    "    train_file = 'BesarKorpus.txt'\n",
    "    #load corpus\n",
    "    token_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "    token_len_char = loadCorpusChar(train_file, bi_dict_char, tri_dict_char, quad_dict_char, vocab_dict_char)\n",
    "    #create the different Nc dictionaries for ngrams\n",
    "    #threshold value\n",
    "    k = 5\n",
    "    V = len(vocab_dict)\n",
    "    tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "    bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "    uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "    tri_nc_dict_char = findFrequencyOfFrequencyCount(tri_dict_char, k, 3, V, len(tri_dict_char))\n",
    "    bi_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 2, V, len(bi_dict_char))\n",
    "    uni_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 1, V, len(vocab_dict_char))\n",
    "    #create bigram probability dictionary\n",
    "    findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, bi_nc_dict, k)\n",
    "    findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k)\n",
    "    findCharBigramProbGT(vocab_dict_char, bi_dict_char, bi_prob_dict_char, bi_nc_dict_char, k)\n",
    "    findCharTrigramProbGT(vocab_dict_char, bi_dict_char, tri_dict_char, tri_prob_dict_char, tri_nc_dict_char, k)\n",
    "    sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "    path_origin = \"C:\\\\Users\\\\User\\\\Downloads\\\\testcase\\\\\"\n",
    "    for file in os.listdir(path_origin):\n",
    "        file = file.split(\"\\\\\")\n",
    "        file_name = file[len(file)-1]\n",
    "        dataCsv.clear()\n",
    "        csvData(file_name)\n",
    "        for data in dataCsv:\n",
    "            sleng_num = 0  \n",
    "            count_char = 0\n",
    "            max_char = 0\n",
    "            input_sen = data\n",
    "            print(input_sen) \n",
    "            print(\"input_sen\")\n",
    "            with conn:\n",
    "                cekWrong(conn,input_sen)  \n",
    "            senInput = input_sen.split()\n",
    "            print(senInput)\n",
    "            print(\"senInput\")\n",
    "            true = []\n",
    "            false = []\n",
    "            for sleng in wrong:\n",
    "                max_char = 0\n",
    "                print(wrong)\n",
    "                print(\"wrong\")\n",
    "                kalimatTeks(sleng_num)\n",
    "                input_sen = input_teks[0]\n",
    "                token = input_sen.split()\n",
    "                print(input_teks)\n",
    "                print(\"input_teks\")\n",
    "                print(token)\n",
    "                print(\"token\")\n",
    "                print(kata[wrong[sleng_num]])\n",
    "                print(\"kata wrong\")\n",
    "#                 Tigram\n",
    "                error = ' '.join(token[1:])\n",
    "                print(error)\n",
    "                print(\"error\")\n",
    "                if error not in tri_prob_dict:\n",
    "                    n=0\n",
    "                else:\n",
    "                    n = len(tri_prob_dict[' '.join(token[1:])])\n",
    "                print(n)\n",
    "                print(\"panjang n\")\n",
    "                for i in range(n):\n",
    "                    prob_trigram_char = 1\n",
    "                    pred = tri_prob_dict[' '.join(token[1:])][i]\n",
    "                    if(contained(pred[1],kata[wrong[sleng_num]])==True):\n",
    "                        b = pred[1]\n",
    "                        print(b)\n",
    "                        trigramchar = [b[j:j+3] for j in range(len(b)-2)]\n",
    "                        for x in range(len(trigramchar)):\n",
    "                            if trigramchar[x] not in tri_prob_dict_char:\n",
    "                                prob_trigram_char = prob_trigram_char * tri_prob_dict_char[trigramchar[x]]\n",
    "                            else:\n",
    "                                prob_trigram_char = prob_trigram_char * tri_prob_dict_char[trigramchar[x]]\n",
    "                        print(pred[1] ,\"%.8f\" % prob_trigram_char)\n",
    "                        print(\"pred dan prob\")\n",
    "                        if prob_trigram_char > max_char:\n",
    "                            max_char = prob_trigram_char\n",
    "                            true.clear()\n",
    "                            true.append(pred[1])\n",
    "                            benar = pred[1]\n",
    "                        if(count_char > 5):\n",
    "                            break\n",
    "                        count_char = count_char + 1\n",
    "                    list_pred.append(pred[1])\n",
    "                if not true:\n",
    "                    true.append(kata[wrong[sleng_num]])\n",
    "                    f = kata[wrong[sleng_num]]\n",
    "                    colomn_wrong.append(f)\n",
    "                    colomn_pred.append(\" \")\n",
    "                    colomn_prob.append(\" \")\n",
    "                else :\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(benar)\n",
    "                    colomn_prob.append(format(max_char,'.9f'))                  \n",
    "        #             bisa dijadikan fungsi    \n",
    "                senInput.pop(sleng)\n",
    "                senInput.insert(sleng,true[0])\n",
    "                print(\" \".join(senInput[:]))\n",
    "                print(\"Gabungan\")\n",
    "                input_teks.clear()\n",
    "                kata.pop(sleng)\n",
    "                kata.insert(sleng,true[0])\n",
    "                sleng_num = sleng_num + 1\n",
    "                true.clear()\n",
    "            print(senInput[:])\n",
    "            print(\"senInput\")\n",
    "            kata.clear()\n",
    "            wrong.clear()\n",
    "            kalimatpred.append(\" \".join(senInput[:]))\n",
    "        test2 = pd.DataFrame(kalimatpred)\n",
    "test2.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarKalimatTrigram%sPred.csv\"%(file_name), index = False, header = False)\n",
    "        kalimatpred.clear()\n",
    "\n",
    "def main():\n",
    "    #variable declaration  \n",
    "    vocab_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "    bi_dict = defaultdict(int)             #for keeping count of sentences of two words\n",
    "    tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "    quad_dict = defaultdict(int)           #for keeping count of sentences of four words\n",
    "    vocab_dict_char = defaultdict(int)          \n",
    "    bi_dict_char = defaultdict(int)             \n",
    "    tri_dict_char = defaultdict(int)           \n",
    "    quad_dict_char = defaultdict(int)           \n",
    "    quad_prob_dict = OrderedDict()              \n",
    "    tri_prob_dict = OrderedDict()\n",
    "    bi_prob_dict = OrderedDict()\n",
    "    quad_prob_dict_char = OrderedDict()              \n",
    "    tri_prob_dict_char = OrderedDict()\n",
    "    bi_prob_dict_char = OrderedDict()\n",
    "    list_pred = []\n",
    "    database = \"C:\\\\db\\\\korpus.db\"\n",
    "    conn = create_connection(database)\n",
    "    #load the corpus for the dataset\n",
    "    train_file = 'BesarKorpus.txt'\n",
    "    #load corpus\n",
    "    token_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "    token_len_char = loadCorpusChar(train_file, bi_dict_char, tri_dict_char, quad_dict_char, vocab_dict_char)\n",
    "    #create the different Nc dictionaries for ngrams\n",
    "    #threshold value\n",
    "    k = 5\n",
    "    V = len(vocab_dict)\n",
    "    tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "    bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "    uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "    tri_nc_dict_char = findFrequencyOfFrequencyCount(tri_dict_char, k, 3, V, len(tri_dict_char))\n",
    "    bi_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 2, V, len(bi_dict_char))\n",
    "    uni_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 1, V, len(vocab_dict_char))\n",
    "    #create bigram probability dictionary\n",
    "    findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, bi_nc_dict, k)\n",
    "    findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k)  \n",
    "    findCharBigramProbGT(vocab_dict_char, bi_dict_char, bi_prob_dict_char, bi_nc_dict_char, k)\n",
    "    findCharTrigramProbGT(vocab_dict_char, bi_dict_char, tri_dict_char, tri_prob_dict_char, tri_nc_dict_char, k)\n",
    "    sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "    path_origin = \"C:\\\\Users\\\\User\\\\Downloads\\\\testcase\\\\\"\n",
    "    for file in os.listdir(path_origin):\n",
    "        file = file.split(\"\\\\\")\n",
    "        file_name = file[len(file)-1]\n",
    "        dataCsv.clear()\n",
    "        csvData(file_name)\n",
    "        for data in dataCsv:\n",
    "            sleng_num = 0  \n",
    "            count_char = 0\n",
    "            max_char = 0\n",
    "            input_sen = data\n",
    "            print(input_sen)  \n",
    "            with conn:\n",
    "                cekWrong(conn,input_sen)  \n",
    "            senInput = input_sen.split()\n",
    "            print(senInput)\n",
    "            true = []\n",
    "            false = []\n",
    "            for sleng in wrong:\n",
    "                max_char = 0\n",
    "                print(wrong)\n",
    "                kalimatTeks(sleng_num)\n",
    "                input_sen = input_teks[0]\n",
    "                token = input_sen.split()\n",
    "                print(input_teks)\n",
    "                print(token)\n",
    "                print(kata[wrong[sleng_num]])\n",
    "                error = ' '.join(token[2:])\n",
    "                if error not in bi_prob_dict:\n",
    "                    n=0\n",
    "                else:\n",
    "                    n = len(bi_prob_dict[' '.join(token[2:])])\n",
    "                print(n)\n",
    "                for i in range(n):\n",
    "                    prob_bigram_char = 1\n",
    "                    pred = bi_prob_dict[' '.join(token[2:])][i]\n",
    "                    if(contained(pred[1],kata[wrong[sleng_num]])==True):\n",
    "                        b = pred[1]\n",
    "\n",
    "                        print(pred[1] ,\"%.8f\" % pred[0])\n",
    "                        print(max_char)\n",
    "                        if prob_bigram_char > max_char:\n",
    "                            max_char = prob_bigram_char\n",
    "                            true.clear()\n",
    "                            true.append(pred[1])\n",
    "                            benar = pred[1]\n",
    "                        if(count_char > 5):\n",
    "                            break\n",
    "                        count_char = count_char + 1\n",
    "                    list_pred.append(pred[1])\n",
    "                if not true:\n",
    "                    true.append(kata[wrong[sleng_num]])\n",
    "                    f = kata[wrong[sleng_num]]\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(\" \")\n",
    "                    colomn_prob.append(\" \")\n",
    "                else :\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(benar)\n",
    "                    colomn_prob.append(format(max_char,'.9f'))\n",
    "        #             bisa dijadikan fungsi    \n",
    "                senInput.pop(sleng)\n",
    "                senInput.insert(sleng,true[0])\n",
    "                print(\" \".join(senInput[:]))\n",
    "                input_teks.clear()\n",
    "                kata.pop(sleng)\n",
    "                kata.insert(sleng,true[0])\n",
    "                sleng_num = sleng_num + 1\n",
    "                true.clear()\n",
    "            print(senInput[:])\n",
    "            kata.clear()\n",
    "            wrong.clear()\n",
    "            kalimatpred.append(\" \".join(senInput[:]))\n",
    "        test = pd.DataFrame(colomn_wrong)\n",
    "test.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarNgramWord%sWrong.csv\"%(file_name), index = False, header = False)  \n",
    "        test1 = pd.DataFrame(colomn_prob)\n",
    "test1.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarNgramWord%sProb.csv\"%(file_name), index = False, header = False)  \n",
    "        test2 = pd.DataFrame(colomn_pred)\n",
    "        test2.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarNgramWord%sPred.csv\"%(file_name), index = False, header = False)\n",
    "        test3 = pd.DataFrame(kalimatpred)\n",
    "test3.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarKalimatNgramWord%sPred.csv\"%(file_name), index = False, header = False)  \n",
    "        kalimatpred.clear()\n",
    "#         print(colomn_wrong)\n",
    "#         print(colomn_pred)\n",
    "#         print(colomn_prob)\n",
    "        colomn_wrong.clear()\n",
    "        colomn_pred.clear()\n",
    "        colomn_prob.clear()\n",
    "\n",
    "def main():\n",
    "    #variable declaration\n",
    "    vocab_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "    bi_dict = defaultdict(int)             #for keeping count of sentences of two words\n",
    "    tri_dict = defaultdict(int)            #for keeping count of sentences of three words\n",
    "    quad_dict = defaultdict(int)           #for keeping count of sentences of four words\n",
    "    vocab_dict_char = defaultdict(int)          \n",
    "    bi_dict_char = defaultdict(int)             \n",
    "    tri_dict_char = defaultdict(int)           \n",
    "    quad_dict_char = defaultdict(int)           \n",
    "    quad_prob_dict = OrderedDict()              \n",
    "    tri_prob_dict = OrderedDict()\n",
    "    bi_prob_dict = OrderedDict()\n",
    "    quad_prob_dict_char = OrderedDict()              \n",
    "    tri_prob_dict_char = OrderedDict()\n",
    "    bi_prob_dict_char = OrderedDict()\n",
    "    list_pred = []\n",
    "    database = \"C:\\\\db\\\\korpus.db\"\n",
    "    conn = create_connection(database)    \n",
    "    #load the corpus for the dataset\n",
    "    train_file = 'BesarKorpus.txt'\n",
    "    #load corpus\n",
    "    token_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\n",
    "    token_len_char = loadCorpusChar(train_file, bi_dict_char, tri_dict_char, quad_dict_char, vocab_dict_char)\n",
    "    #create the different Nc dictionaries for ngrams\n",
    "    #threshold value\n",
    "    k = 5\n",
    "    V = len(vocab_dict)\n",
    "    tri_nc_dict = findFrequencyOfFrequencyCount(tri_dict, k, 3, V, len(tri_dict))\n",
    "    bi_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 2, V, len(bi_dict))\n",
    "    uni_nc_dict = findFrequencyOfFrequencyCount(bi_dict, k, 1, V, len(vocab_dict))\n",
    "    tri_nc_dict_char = findFrequencyOfFrequencyCount(tri_dict_char, k, 3, V, len(tri_dict_char))\n",
    "    bi_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 2, V, len(bi_dict_char))\n",
    "    uni_nc_dict_char = findFrequencyOfFrequencyCount(bi_dict_char, k, 1, V, len(vocab_dict_char))\n",
    "    #create bigram probability dictionary\n",
    "    findBigramProbGT(vocab_dict, bi_dict, bi_prob_dict, bi_nc_dict, k)\n",
    "    findTrigramProbGT(vocab_dict, bi_dict, tri_dict, tri_prob_dict, tri_nc_dict, k)\n",
    "    findCharBigramProbGT(vocab_dict_char, bi_dict_char, bi_prob_dict_char, bi_nc_dict_char, k)\n",
    "    findCharTrigramProbGT(vocab_dict_char, bi_dict_char, tri_dict_char, tri_prob_dict_char, tri_nc_dict_char, k)\n",
    "    sortProbWordDict(bi_prob_dict, tri_prob_dict, quad_prob_dict)\n",
    "    path_origin = \"C:\\\\Users\\\\User\\\\Downloads\\\\testcase\\\\\"\n",
    "    for file in os.listdir(path_origin):\n",
    "        file = file.split(\"\\\\\")\n",
    "        file_name = file[len(file)-1]\n",
    "        dataCsv.clear()\n",
    "        csvData(file_name)\n",
    "        for data in dataCsv:\n",
    "            sleng_num = 0  \n",
    "            count_char = 0\n",
    "            max_char = 0\n",
    "            input_sen = data\n",
    "            print(input_sen) \n",
    "            print(\"input_sen\")\n",
    "            with conn:\n",
    "                cekWrong(conn,input_sen)  \n",
    "            senInput = input_sen.split()\n",
    "            print(senInput)\n",
    "            print(\"senInput\")\n",
    "            true = []\n",
    "            false = []\n",
    "            for sleng in wrong:\n",
    "                max_char = 0\n",
    "                print(wrong)\n",
    "                print(\"wrong\")\n",
    "                kalimatTeks(sleng_num)\n",
    "                input_sen = input_teks[0]\n",
    "                token = input_sen.split()\n",
    "                print(input_teks)\n",
    "                print(\"input_teks\")\n",
    "                print(token)\n",
    "                print(\"token\")\n",
    "                print(kata[wrong[sleng_num]])\n",
    "                print(\"kata wrong\")\n",
    "#                 Bigram\n",
    "#                 Tigram\n",
    "                error = ' '.join(token[1:])\n",
    "                print(error)\n",
    "                print(\"error\")\n",
    "                if error not in tri_prob_dict:\n",
    "                    n=0\n",
    "                else:\n",
    "                    n = len(tri_prob_dict[' '.join(token[1:])])\n",
    "                print(n)\n",
    "                print(\"panjang n\")\n",
    "                for i in range(n):\n",
    "                    pred = tri_prob_dict[' '.join(token[1:])][i]\n",
    "                    prob_trigram_char = pred[0]\n",
    "                    if(contained(pred[1],kata[wrong[sleng_num]])==True):\n",
    "                        b = pred[1]\n",
    "                        print(b)\n",
    "                        print(pred[1] ,\"%.8f\" % pred[0])\n",
    "                        print(\"pred dan prob\")\n",
    "                        if prob_trigram_char > max_char:\n",
    "                            max_char = prob_trigram_char\n",
    "                            true.clear()\n",
    "                            true.append(pred[1])\n",
    "                            benar = pred[1]\n",
    "                        if(count_char > 5):\n",
    "                            break\n",
    "                        count_char = count_char + 1\n",
    "        #                 else :\n",
    "        #                     print(pred[1] , \"False\")\n",
    "                    list_pred.append(pred[1])\n",
    "                if not true:\n",
    "                    true.append(kata[wrong[sleng_num]])\n",
    "                    f = kata[wrong[sleng_num]]\n",
    "                    colomn_wrong.append(f)\n",
    "                    colomn_pred.append(\" \")\n",
    "                    colomn_prob.append(\" \")\n",
    "                else :\n",
    "                    colomn_wrong.append(kata[wrong[sleng_num]])\n",
    "                    colomn_pred.append(benar)\n",
    "                    colomn_prob.append(format(max_char,'.9f'))                  \n",
    "        #             bisa dijadikan fungsi    \n",
    "                senInput.pop(sleng)\n",
    "                senInput.insert(sleng,true[0])\n",
    "                print(\" \".join(senInput[:]))\n",
    "                print(\"Gabungan\")\n",
    "                input_teks.clear()\n",
    "                kata.pop(sleng)\n",
    "                kata.insert(sleng,true[0])\n",
    "                sleng_num = sleng_num + 1\n",
    "                true.clear()\n",
    "            print(senInput[:])\n",
    "            print(\"senInput\")\n",
    "            kata.clear()\n",
    "            wrong.clear()\n",
    "            kalimatpred.append(\" \".join(senInput[:]))\n",
    "        test1 = pd.DataFrame(colomn_prob)\n",
    "     test1.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarTrigram%sProb.csv\"%(file_name), index = False, header = False)  \n",
    "        test2 = pd.DataFrame(colomn_pred)\n",
    "  test2.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarTrigram%sPred.csv\"%(file_name), index = False, header = False)  \n",
    "        test3 = pd.DataFrame(kalimatpred)\n",
    "test3.to_csv(\"C:\\\\Users\\\\User\\\\Downloads\\\\KorpusBesarKalimatTrigramNgramWord%sPred.csv\"%(file_name), index = False, header = False)\n",
    "        kalimatpred.clear()\n",
    "        colomn_wrong.clear()\n",
    "        colomn_pred.clear()\n",
    "        colomn_prob.clear()\n"
   ]
  }
 ]
}